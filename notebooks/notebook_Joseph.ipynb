{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joseph Korban's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: DL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume BTC</th>\n",
       "      <th>volume USD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>795.150377</td>\n",
       "      <td>3.454770e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>1199.888264</td>\n",
       "      <td>5.086958e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>381.309763</td>\n",
       "      <td>1.549484e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4139.98</td>\n",
       "      <td>4211.08</td>\n",
       "      <td>4032.62</td>\n",
       "      <td>4086.29</td>\n",
       "      <td>467.083022</td>\n",
       "      <td>1.930364e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4069.13</td>\n",
       "      <td>4119.62</td>\n",
       "      <td>3911.79</td>\n",
       "      <td>4016.00</td>\n",
       "      <td>691.743060</td>\n",
       "      <td>2.797232e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      open     high      low    close   volume BTC    volume USD\n",
       "0  4261.48  4485.39  4200.74  4285.08   795.150377  3.454770e+06\n",
       "1  4285.08  4371.52  3938.77  4108.37  1199.888264  5.086958e+06\n",
       "2  4108.37  4184.69  3850.00  4139.98   381.309763  1.549484e+06\n",
       "3  4139.98  4211.08  4032.62  4086.29   467.083022  1.930364e+06\n",
       "4  4069.13  4119.62  3911.79  4016.00   691.743060  2.797232e+06"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv('/Users/joekorban/code/J0eDub/crypto_market_assistant/Raw Data/BTCUSDT_daily_Binance.csv')\n",
    "\n",
    "df_droptime = df.drop(columns='time')\n",
    "\n",
    "df_droptime.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments: \n",
    "- Targets  \n",
    "- Number of targets \n",
    "- Number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can change later\n",
    "\n",
    "N_FEATURES = df_droptime.shape[1]  # All features other than time\n",
    "N_TARGETS = 1                      # Prediciting only 1 target (close price)\n",
    "TARGET = 'close'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for DL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Creating many folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments: \n",
    "- Fold Length   \n",
    "- Fold Size\n",
    "- Train Test Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can change later\n",
    "\n",
    "# Since we have data for about 6.3 years, we will asssume fold lengths of 2 years\n",
    "# 1.34 years for training and 0.67 for testing\n",
    "\n",
    "FOLD_LENGTH = 365 * 2     # Assume fold legnth of 2 years\n",
    "FOLD_STRIDE = 91          # Assume stride every quarter\n",
    "TRAIN_TEST_RATIO = 0.67   # Two-thirds split ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Implement the Get Folds Function:\n",
    "- Takes in df, fold length, fold stride\n",
    "- Returns a list of folds (each fold being a dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through the dataframe and creates equal sized folds of FOLD_LENGTH\n",
    "# starting at 0 and at each FOLD_STRIDE until it crosses len(df)\n",
    "\n",
    "def get_folds(df: pd.DataFrame,\n",
    "    fold_length: int,\n",
    "    fold_stride: int) -> list[pd.DataFrame]:\n",
    "\n",
    "    folds = []\n",
    "    for index in range(0, fold_length, fold_stride):\n",
    "        if index + fold_length > len(df):\n",
    "            break\n",
    "        fold = df.iloc[index:index + fold_length,:]\n",
    "        folds.append(fold)\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds created were 9\n",
      "Each with a shape equal to (730, 6).\n"
     ]
    }
   ],
   "source": [
    "folds = (get_folds(df_droptime,FOLD_LENGTH,FOLD_STRIDE))\n",
    "print(f'Number of folds created were {len(folds)}')\n",
    "print(f'Each with a shape equal to {folds[0].shape}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Temporal Train Test Split for one fold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focusing on one fold first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = folds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments: \n",
    "- Input Length : Selected period selected for forecasting\n",
    "- Output Length : Selected period we want to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can change later\n",
    "\n",
    "INPUT_LENGTH = 30           # We can assume one month for a forecating period\n",
    "OUTPUT_LENGTH = 1           # If we want predict only one timestep (day) ahead\n",
    "TEMP_TRAIN_TEST_RATIO = 0.8 # How we want to split each fold (can be same as train test ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Implement the Temporal Train Test Split Function:\n",
    "- Takes in a fold, temporal train test ratio, input length\n",
    "- Returns a tuple of (fold_train, fold_test), each as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting one fold into train and test chronologically,\n",
    "# so we can later sample many (Xi,yi) pairs (input and output lengths)\n",
    "\n",
    "def temporal_train_test_split(fold:pd.DataFrame,\n",
    "                            temp_train_test_ratio: float,\n",
    "                            input_length: int) -> tuple[pd.DataFrame]:\n",
    "    # Train set\n",
    "    last_train_index = round(temp_train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_index, :]\n",
    "\n",
    "    #Test Set\n",
    "    first_test_index = last_train_index - input_length\n",
    "    fold_test = fold.iloc[first_test_index:, :]\n",
    "\n",
    "    return (fold_train,fold_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fold_train, fold_test) = temporal_train_test_split(fold, TEMP_TRAIN_TEST_RATIO, INPUT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((584, 6), (176, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_train.shape, fold_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create (X, y) sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we split our fold into trian and test we have to create 2 functions \n",
    "- One that will return a single sequence of (Xi, yi) (from fold train for training, and from fold test for evaluating)\n",
    "- Another that will call the first function many times over creating 3-D arrays of multiple sequences (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Implement the Get (Xi, yi) function (using the random method):\n",
    "- Takes in a fold, input length ,output length\n",
    "- Returns a single sequence of (Xi, yi) as a 2-D DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a fold (could be train fold for testing or test fold for evaluating),\n",
    "# it will return a sequence of (Xi,yi) based on input and output lengths,\n",
    "# starting from a random point.\n",
    "\n",
    "def get_Xi_yi(fold:pd.DataFrame,\n",
    "              input_length:int,\n",
    "              output_length:int):\n",
    "\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(fold) - (input_length + output_length) + 1\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "    X_i = fold.iloc[random_start:random_start+input_length]\n",
    "    y_i = fold.iloc[random_start+input_length:\n",
    "                  random_start+input_length+output_length][[TARGET]]\n",
    "\n",
    "    return (X_i, y_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_i, y_train_i = get_Xi_yi(fold_train, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_test_i, y_test_i = get_Xi_yi(fold_test, INPUT_LENGTH, OUTPUT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 6), (1, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_i.shape, y_train_i.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 6), (1, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_i.shape, y_test_i.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments: \n",
    "- Number of sequences train : How many sequences do we want to collect from X_train\n",
    "- Number of sequences test : How many sequences do we want to collect from X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can change later\n",
    "\n",
    "N_TRAIN = 6666 # number_of_sequences_train\n",
    "N_TEST =  3333 # number_of_sequences_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Implement the Get (X,y) Function:\n",
    "- Takes in fold, number of sequences, input length, output length\n",
    "- Returns (X, y) as a 3-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the previous get_Xi_yi function and loops based on number of sequences,\n",
    "# and returns your complete X_train, y_train as 3-D arrays of\n",
    "#( Number of sequences, time steps, number of features(targets) )\n",
    "\n",
    "def get_X_y(fold:pd.DataFrame,\n",
    "            number_of_sequences:int,\n",
    "            input_length:int,\n",
    "            output_length:int):\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(number_of_sequences):\n",
    "        (Xi, yi) = get_Xi_yi(fold, input_length, output_length)\n",
    "        X.append(Xi)\n",
    "        y.append(yi)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(fold_train, N_TRAIN, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_test, y_test = get_X_y(fold_test, N_TEST, INPUT_LENGTH, OUTPUT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6666, 30, 6), (6666, 1, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3333, 30, 6), (3333, 1, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Building a Baseline (dummy) model for comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Initialize the Baseline Model function based on last seen value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_baseline():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Lambda(lambda x: x[:,-1,1,None]))\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=0.02)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[\"mse\",'mae'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88/209 [===========>..................] - ETA: 0s - loss: 363063.8438 - mse: 363063.8438 - mae: 350.5678 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 0s 575us/step - loss: 377755.4688 - mse: 377755.4688 - mae: 358.0751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16abe9150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = init_baseline()\n",
    "base_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 529us/step - loss: 306864.7812 - mse: 306864.7812 - mae: 357.0074\n"
     ]
    }
   ],
   "source": [
    "baseline_score = base_model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Building an LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Initialize the LSTM Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(X_train, y_train):\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    output_length = y_train.shape[1]*y_train.shape[2]\n",
    "\n",
    "    normalizer = Normalization()\n",
    "    normalizer.adapt(X_train)\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(normalizer)\n",
    "    model.add(layers.LSTM(250,\n",
    "                          activation='tanh',\n",
    "                          return_sequences = True,\n",
    "                          ))\n",
    "    model.add(layers.LSTM(125,\n",
    "                          activation='tanh',\n",
    "                          return_sequences = True,\n",
    "                          ))\n",
    "    model.add(layers.LSTM(50,\n",
    "                          activation='tanh',\n",
    "                          return_sequences = True,\n",
    "                          ))\n",
    "    model.add(layers.LSTM(75,\n",
    "                          activation='tanh',\n",
    "                          return_sequences = False,\n",
    "                          ))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(output_length, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[\"mse\",'mae'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Fit (Train) the LSTM Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model: tf.keras.Model, verbose=1) -> tuple[tf.keras.Model, dict]:\n",
    "\n",
    "    es = EarlyStopping(monitor = \"val_loss\",\n",
    "                      patience = 15,\n",
    "                      mode = \"min\",\n",
    "                      restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_split = 0.3,\n",
    "                        shuffle = False,\n",
    "                        batch_size = 32,\n",
    "                        epochs = 100,\n",
    "                        callbacks = [es],\n",
    "                        verbose = verbose)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Define the plot history function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  (None, None, 6)           13        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 250)         257000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 125)         188000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 50)          35200     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 75)                37800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                760       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 518784 (1.98 MB)\n",
      "Trainable params: 518771 (1.98 MB)\n",
      "Non-trainable params: 13 (56.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 16s 88ms/step - loss: 52624516.0000 - mse: 52624516.0000 - mae: 6549.8291 - val_loss: 41248824.0000 - val_mse: 41248824.0000 - val_mae: 5515.6611\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 23000534.0000 - mse: 23000534.0000 - mae: 3716.8589 - val_loss: 13785004.0000 - val_mse: 13785004.0000 - val_mae: 2668.4287\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 10091609.0000 - mse: 10091609.0000 - mae: 2240.9243 - val_loss: 10865490.0000 - val_mse: 10865490.0000 - val_mae: 2378.9072\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 8452117.0000 - mse: 8452117.0000 - mae: 1945.8618 - val_loss: 6582006.5000 - val_mse: 6582006.5000 - val_mae: 1474.4767\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 4056262.7500 - mse: 4056262.7500 - mae: 1152.2249 - val_loss: 3364258.7500 - val_mse: 3364258.7500 - val_mae: 971.2995\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 1967951.5000 - mse: 1967951.5000 - mae: 740.1910 - val_loss: 1943231.3750 - val_mse: 1943231.3750 - val_mae: 725.4090\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 1170925.3750 - mse: 1170925.3750 - mae: 591.4501 - val_loss: 1222891.3750 - val_mse: 1222891.3750 - val_mae: 585.6528\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 920347.1875 - mse: 920347.1875 - mae: 578.0434 - val_loss: 1059321.0000 - val_mse: 1059321.0000 - val_mae: 620.7929\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 2268230.7500 - mse: 2268230.7500 - mae: 933.7216 - val_loss: 2628348.2500 - val_mse: 2628348.2500 - val_mae: 1237.7410\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 1370963.6250 - mse: 1370963.6250 - mae: 808.2508 - val_loss: 1007788.8125 - val_mse: 1007788.8125 - val_mae: 555.5977\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 643442.8125 - mse: 643442.8125 - mae: 492.8568 - val_loss: 684675.3750 - val_mse: 684675.3750 - val_mae: 443.9776\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 469835.9375 - mse: 469835.9375 - mae: 411.9793 - val_loss: 519581.2500 - val_mse: 519581.2500 - val_mae: 397.9485\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 436062.8750 - mse: 436062.8750 - mae: 418.9699 - val_loss: 520012.1562 - val_mse: 520012.1562 - val_mae: 440.8363\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 366865.9375 - mse: 366865.9375 - mae: 380.0540 - val_loss: 361421.3750 - val_mse: 361421.3750 - val_mae: 345.9559\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 285425.3125 - mse: 285425.3125 - mae: 333.5899 - val_loss: 319517.5312 - val_mse: 319517.5312 - val_mae: 330.0824\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 315532.4688 - mse: 315532.4688 - mae: 355.8677 - val_loss: 643302.7500 - val_mse: 643302.7500 - val_mae: 578.3437\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 401044.5938 - mse: 401044.5938 - mae: 422.7379 - val_loss: 301211.6250 - val_mse: 301211.6250 - val_mae: 343.8502\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 274956.0000 - mse: 274956.0000 - mae: 342.4660 - val_loss: 275454.4062 - val_mse: 275454.4062 - val_mae: 330.9110\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 13s 89ms/step - loss: 257835.5312 - mse: 257835.5312 - mae: 332.8620 - val_loss: 321958.3125 - val_mse: 321958.3125 - val_mae: 403.0206\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 216128.0781 - mse: 216128.0781 - mae: 297.5884 - val_loss: 241266.3594 - val_mse: 241266.3594 - val_mae: 284.5735\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 209175.3750 - mse: 209175.3750 - mae: 290.8219 - val_loss: 250567.0469 - val_mse: 250567.0469 - val_mae: 316.9661\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 305003.9375 - mse: 305003.9375 - mae: 357.7853 - val_loss: 417360.1875 - val_mse: 417360.1875 - val_mae: 445.3386\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 275484.0000 - mse: 275484.0000 - mae: 341.5182 - val_loss: 242336.5156 - val_mse: 242336.5156 - val_mae: 336.7712\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 235117.4844 - mse: 235117.4844 - mae: 319.4404 - val_loss: 249775.4531 - val_mse: 249775.4531 - val_mae: 285.6516\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 192535.2500 - mse: 192535.2500 - mae: 285.7432 - val_loss: 201108.3438 - val_mse: 201108.3438 - val_mae: 289.6493\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 257712.3906 - mse: 257712.3906 - mae: 335.8312 - val_loss: 341286.5938 - val_mse: 341286.5938 - val_mae: 367.7839\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 228979.9219 - mse: 228979.9219 - mae: 314.2272 - val_loss: 172934.6562 - val_mse: 172934.6562 - val_mae: 265.9190\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 156574.8281 - mse: 156574.8281 - mae: 267.5768 - val_loss: 176756.1719 - val_mse: 176756.1719 - val_mae: 283.8018\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 126411.7422 - mse: 126411.7422 - mae: 239.7877 - val_loss: 142723.3750 - val_mse: 142723.3750 - val_mae: 247.4724\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 164693.7969 - mse: 164693.7969 - mae: 271.9970 - val_loss: 196806.4219 - val_mse: 196806.4219 - val_mae: 286.2500\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 195531.5156 - mse: 195531.5156 - mae: 295.0208 - val_loss: 253423.8906 - val_mse: 253423.8906 - val_mae: 331.6071\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 186690.0156 - mse: 186690.0156 - mae: 294.2462 - val_loss: 225186.8281 - val_mse: 225186.8281 - val_mae: 356.5862\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 618853.6875 - mse: 618853.6875 - mae: 454.3731 - val_loss: 2056972.3750 - val_mse: 2056972.3750 - val_mae: 1032.1763\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 611205.3750 - mse: 611205.3750 - mae: 507.1995 - val_loss: 484821.2188 - val_mse: 484821.2188 - val_mae: 434.7354\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 314678.3125 - mse: 314678.3125 - mae: 372.2221 - val_loss: 218681.7344 - val_mse: 218681.7344 - val_mae: 296.5497\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 220174.8750 - mse: 220174.8750 - mae: 310.6025 - val_loss: 188650.2500 - val_mse: 188650.2500 - val_mae: 313.8188\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 176988.4375 - mse: 176988.4375 - mae: 282.2686 - val_loss: 190019.1562 - val_mse: 190019.1562 - val_mae: 299.8565\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 112608.0859 - mse: 112608.0859 - mae: 228.9268 - val_loss: 152626.8906 - val_mse: 152626.8906 - val_mae: 261.0031\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 105392.6328 - mse: 105392.6328 - mae: 231.5125 - val_loss: 126239.8516 - val_mse: 126239.8516 - val_mae: 237.0888\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 108080.2031 - mse: 108080.2031 - mae: 224.7094 - val_loss: 105394.1094 - val_mse: 105394.1094 - val_mae: 212.9266\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 76862.2500 - mse: 76862.2500 - mae: 200.6763 - val_loss: 94718.0469 - val_mse: 94718.0469 - val_mae: 223.8652\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 68029.6172 - mse: 68029.6172 - mae: 190.4067 - val_loss: 56325.0273 - val_mse: 56325.0273 - val_mae: 180.5297\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 38803.0742 - mse: 38803.0742 - mae: 149.3543 - val_loss: 40462.6055 - val_mse: 40462.6055 - val_mae: 155.1524\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 29912.6211 - mse: 29912.6211 - mae: 129.9037 - val_loss: 31929.4062 - val_mse: 31929.4062 - val_mae: 129.3656\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 23818.5566 - mse: 23818.5566 - mae: 116.0782 - val_loss: 26816.2129 - val_mse: 26816.2129 - val_mae: 122.2718\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 26726.7188 - mse: 26726.7188 - mae: 121.9518 - val_loss: 26307.4766 - val_mse: 26307.4766 - val_mae: 124.2096\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 18s 124ms/step - loss: 23095.3652 - mse: 23095.3652 - mae: 112.9153 - val_loss: 20071.0469 - val_mse: 20071.0469 - val_mae: 107.2607\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 22918.4121 - mse: 22918.4121 - mae: 113.0669 - val_loss: 29729.7285 - val_mse: 29729.7285 - val_mae: 114.7288\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 71885.2266 - mse: 71885.2266 - mae: 166.3330 - val_loss: 231130.7188 - val_mse: 231130.7188 - val_mae: 290.6879\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 275666.5938 - mse: 275666.5938 - mae: 335.9578 - val_loss: 198303.4219 - val_mse: 198303.4219 - val_mae: 288.9985\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 209795.1562 - mse: 209795.1562 - mae: 313.4818 - val_loss: 206267.2500 - val_mse: 206267.2500 - val_mae: 289.4452\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 180525.5000 - mse: 180525.5000 - mae: 282.5214 - val_loss: 180657.3594 - val_mse: 180657.3594 - val_mae: 279.1548\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 136956.2656 - mse: 136956.2656 - mae: 243.3494 - val_loss: 110155.9531 - val_mse: 110155.9531 - val_mae: 224.7647\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 102599.2109 - mse: 102599.2109 - mae: 217.1729 - val_loss: 90552.6094 - val_mse: 90552.6094 - val_mae: 195.1936\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 79s 547ms/step - loss: 122558.7656 - mse: 122558.7656 - mae: 233.1816 - val_loss: 114330.9609 - val_mse: 114330.9609 - val_mae: 204.1024\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 101640.6719 - mse: 101640.6719 - mae: 221.4950 - val_loss: 115102.1719 - val_mse: 115102.1719 - val_mae: 233.0603\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 191808.8125 - mse: 191808.8125 - mae: 284.9545 - val_loss: 241569.1562 - val_mse: 241569.1562 - val_mae: 356.6358\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 128433.4062 - mse: 128433.4062 - mae: 250.8513 - val_loss: 92742.7500 - val_mse: 92742.7500 - val_mae: 206.7368\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 111479.6797 - mse: 111479.6797 - mae: 221.4369 - val_loss: 136511.4219 - val_mse: 136511.4219 - val_mae: 247.3169\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 121934.5234 - mse: 121934.5234 - mae: 233.4917 - val_loss: 91276.9688 - val_mse: 91276.9688 - val_mae: 199.3295\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 72922.0078 - mse: 72922.0078 - mae: 185.3145 - val_loss: 103407.9609 - val_mse: 103407.9609 - val_mae: 201.8252\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 75726.2891 - mse: 75726.2891 - mae: 188.2359 - val_loss: 92724.7656 - val_mse: 92724.7656 - val_mae: 174.2178\n"
     ]
    }
   ],
   "source": [
    "model = init_model(X_train, y_train)\n",
    "model.summary()\n",
    "\n",
    "model, history = fit_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<Axes: title={'center': 'MSE'}, xlabel='Epoch', ylabel='Loss'>,\n",
       "       <Axes: title={'center': 'MAE'}, xlabel='Epoch', ylabel='MAE'>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 2s 24ms/step - loss: 790492.0000 - mse: 790492.0000 - mae: 692.8980\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All this was for one fold, now we need to Cross validate on all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Funtion to Cross Validate for Baseline and LSTM Models in for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline_and_lstm():\n",
    "    '''\n",
    "    This function cross-validates\n",
    "    - the \"last seen value\" baseline model\n",
    "    - the RNN model\n",
    "    '''\n",
    "\n",
    "    list_of_mae_baseline_model = []\n",
    "    list_of_mae_recurrent_model = []\n",
    "\n",
    "    # 0 - Creating folds\n",
    "    # =========================================\n",
    "    folds = get_folds(df_droptime, FOLD_LENGTH, FOLD_STRIDE)\n",
    "\n",
    "    for fold_id, fold in enumerate(folds):\n",
    "\n",
    "        # 1 - Train/Test split the current fold\n",
    "\n",
    "        (fold_train, fold_test) = temporal_train_test_split(fold, TRAIN_TEST_RATIO, INPUT_LENGTH)\n",
    "\n",
    "        X_train, y_train = get_X_y(fold_train, N_TRAIN, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "        X_test, y_test = get_X_y(fold_test, N_TEST, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "\n",
    "        # 2 - Modelling\n",
    "\n",
    "        ##### Baseline Model\n",
    "        baseline_model = init_baseline()\n",
    "        mae_baseline = baseline_model.evaluate(X_test, y_test, verbose=0)[2]\n",
    "        list_of_mae_baseline_model.append(mae_baseline)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")\n",
    "\n",
    "        ##### LSTM Model\n",
    "        model = init_model(X_train, y_train)\n",
    "        history = fit_model(model)\n",
    "        # es = EarlyStopping(monitor = \"val_mae\",\n",
    "        #                    mode = \"min\",\n",
    "        #                    patience = 2,\n",
    "        #                    restore_best_weights = True)\n",
    "        # history = model.fit(X_train, y_train,\n",
    "        #                     validation_split = 0.3,\n",
    "        #                     shuffle = False,\n",
    "        #                     batch_size = 32,\n",
    "        #                     epochs = 50,\n",
    "        #                     callbacks = [es],\n",
    "        #                     verbose = 0)\n",
    "        res = model.evaluate(X_test, y_test, verbose=0)\n",
    "        mae_lstm = res[2]\n",
    "        list_of_mae_recurrent_model.append(mae_lstm)\n",
    "        print(f\"MAE LSTM fold n°{fold_id} = {round(mae_lstm, 2)}\")\n",
    "\n",
    "        ##### Comparison LSTM vs Baseline for the current fold\n",
    "        print(f\"🏋🏽‍♂️ improvement over baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")\n",
    "\n",
    "    return list_of_mae_baseline_model, list_of_mae_recurrent_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°0 = 256.21\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 18s 101ms/step - loss: 49229072.0000 - mse: 49229072.0000 - mae: 6252.5454 - val_loss: 31142864.0000 - val_mse: 31142864.0000 - val_mae: 4507.3857\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 15141133.0000 - mse: 15141133.0000 - mae: 2807.4680 - val_loss: 11030685.0000 - val_mse: 11030685.0000 - val_mae: 2342.8779\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 9529016.0000 - mse: 9529016.0000 - mae: 2236.5776 - val_loss: 10831817.0000 - val_mse: 10831817.0000 - val_mae: 2403.2678\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 9475884.0000 - mse: 9475884.0000 - mae: 2220.9141 - val_loss: 11001474.0000 - val_mse: 11001474.0000 - val_mae: 2346.8921\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 9430070.0000 - mse: 9430070.0000 - mae: 2213.4307 - val_loss: 9683945.0000 - val_mse: 9683945.0000 - val_mae: 2154.4050\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 7312072.5000 - mse: 7312072.5000 - mae: 1901.7198 - val_loss: 7126978.5000 - val_mse: 7126978.5000 - val_mae: 1828.6458\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 5284523.5000 - mse: 5284523.5000 - mae: 1626.2070 - val_loss: 5648190.0000 - val_mse: 5648190.0000 - val_mae: 1623.0176\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 3800262.7500 - mse: 3800262.7500 - mae: 1325.4109 - val_loss: 2183976.5000 - val_mse: 2183976.5000 - val_mae: 819.1226\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 1309466.1250 - mse: 1309466.1250 - mae: 664.7522 - val_loss: 1052044.2500 - val_mse: 1052044.2500 - val_mae: 517.7651\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 710760.6875 - mse: 710760.6875 - mae: 499.8046 - val_loss: 729297.4375 - val_mse: 729297.4375 - val_mae: 468.8891\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 527103.7500 - mse: 527103.7500 - mae: 441.7198 - val_loss: 538781.1250 - val_mse: 538781.1250 - val_mae: 419.0539\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 418319.2812 - mse: 418319.2812 - mae: 401.5541 - val_loss: 549285.8125 - val_mse: 549285.8125 - val_mae: 449.1353\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 381377.3438 - mse: 381377.3438 - mae: 376.7836 - val_loss: 422628.7812 - val_mse: 422628.7812 - val_mae: 370.4421\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 298077.9062 - mse: 298077.9062 - mae: 328.6242 - val_loss: 322147.7188 - val_mse: 322147.7188 - val_mae: 320.6960\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 349643.2500 - mse: 349643.2500 - mae: 373.3730 - val_loss: 383133.5938 - val_mse: 383133.5938 - val_mae: 378.8647\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 17s 118ms/step - loss: 313513.4375 - mse: 313513.4375 - mae: 337.3676 - val_loss: 293021.4688 - val_mse: 293021.4688 - val_mae: 324.9478\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 18s 122ms/step - loss: 393758.6562 - mse: 393758.6562 - mae: 376.6118 - val_loss: 641022.0000 - val_mse: 641022.0000 - val_mae: 477.1837\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 18s 126ms/step - loss: 406045.3438 - mse: 406045.3438 - mae: 386.1894 - val_loss: 407256.8125 - val_mse: 407256.8125 - val_mae: 380.7503\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 19s 131ms/step - loss: 329354.9688 - mse: 329354.9688 - mae: 362.5390 - val_loss: 309503.2188 - val_mse: 309503.2188 - val_mae: 318.6362\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 19s 128ms/step - loss: 249880.3125 - mse: 249880.3125 - mae: 311.5622 - val_loss: 234762.1562 - val_mse: 234762.1562 - val_mae: 287.7603\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 211293.7188 - mse: 211293.7188 - mae: 297.8800 - val_loss: 206222.9375 - val_mse: 206222.9375 - val_mae: 276.9118\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 157899.8125 - mse: 157899.8125 - mae: 252.5694 - val_loss: 161300.7969 - val_mse: 161300.7969 - val_mae: 251.0663\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 18s 122ms/step - loss: 150697.6250 - mse: 150697.6250 - mae: 252.0435 - val_loss: 157416.2500 - val_mse: 157416.2500 - val_mae: 259.2704\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 138610.5781 - mse: 138610.5781 - mae: 241.8058 - val_loss: 127153.0625 - val_mse: 127153.0625 - val_mae: 226.0841\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 18s 121ms/step - loss: 134334.9219 - mse: 134334.9219 - mae: 240.7015 - val_loss: 148714.2344 - val_mse: 148714.2344 - val_mae: 251.0074\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 142491.6562 - mse: 142491.6562 - mae: 255.0301 - val_loss: 124469.2109 - val_mse: 124469.2109 - val_mae: 230.9533\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 18s 124ms/step - loss: 115797.3047 - mse: 115797.3047 - mae: 230.4288 - val_loss: 140067.5469 - val_mse: 140067.5469 - val_mae: 237.0704\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 17s 113ms/step - loss: 114833.7500 - mse: 114833.7500 - mae: 225.1634 - val_loss: 96506.6016 - val_mse: 96506.6016 - val_mae: 211.3921\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 118834.4375 - mse: 118834.4375 - mae: 229.4115 - val_loss: 133213.1094 - val_mse: 133213.1094 - val_mae: 245.7245\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 138126.8281 - mse: 138126.8281 - mae: 247.7941 - val_loss: 149758.5781 - val_mse: 149758.5781 - val_mae: 248.5335\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 127020.1953 - mse: 127020.1953 - mae: 234.1698 - val_loss: 107301.3516 - val_mse: 107301.3516 - val_mae: 212.9019\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 146702.2969 - mse: 146702.2969 - mae: 240.4085 - val_loss: 215361.6406 - val_mse: 215361.6406 - val_mae: 286.2403\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 16s 106ms/step - loss: 128173.4531 - mse: 128173.4531 - mae: 240.8113 - val_loss: 151322.7344 - val_mse: 151322.7344 - val_mae: 248.9765\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 255207.0156 - mse: 255207.0156 - mae: 323.2799 - val_loss: 263044.2188 - val_mse: 263044.2188 - val_mae: 348.9072\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 248942.3281 - mse: 248942.3281 - mae: 327.3354 - val_loss: 293120.5312 - val_mse: 293120.5312 - val_mae: 356.4804\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 16s 113ms/step - loss: 320742.4062 - mse: 320742.4062 - mae: 374.1746 - val_loss: 331484.3125 - val_mse: 331484.3125 - val_mae: 359.8679\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 231880.8906 - mse: 231880.8906 - mae: 322.2784 - val_loss: 264198.2812 - val_mse: 264198.2812 - val_mae: 377.3502\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 195093.3906 - mse: 195093.3906 - mae: 282.3924 - val_loss: 224674.7031 - val_mse: 224674.7031 - val_mae: 331.9297\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 18s 121ms/step - loss: 183856.8438 - mse: 183856.8438 - mae: 283.2594 - val_loss: 330679.0625 - val_mse: 330679.0625 - val_mae: 340.5886\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 18s 125ms/step - loss: 175594.2500 - mse: 175594.2500 - mae: 285.3167 - val_loss: 140776.0781 - val_mse: 140776.0781 - val_mae: 244.0143\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 19s 130ms/step - loss: 125992.7656 - mse: 125992.7656 - mae: 240.3987 - val_loss: 138098.7969 - val_mse: 138098.7969 - val_mae: 216.8894\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 19s 134ms/step - loss: 126244.9766 - mse: 126244.9766 - mae: 239.5987 - val_loss: 166158.9062 - val_mse: 166158.9062 - val_mae: 265.6782\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 18s 127ms/step - loss: 116591.6875 - mse: 116591.6875 - mae: 225.5044 - val_loss: 96771.2656 - val_mse: 96771.2656 - val_mae: 200.4382\n",
      "MAE LSTM fold n°0 = 653.2\n",
      "🏋🏽‍♂️ improvement over baseline: -154.95 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°1 = 345.39\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 18s 105ms/step - loss: 53934692.0000 - mse: 53934692.0000 - mae: 6655.3374 - val_loss: 45009968.0000 - val_mse: 45009968.0000 - val_mae: 5846.6768\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 27230836.0000 - mse: 27230836.0000 - mae: 4170.4858 - val_loss: 16800862.0000 - val_mse: 16800862.0000 - val_mae: 2983.3723\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 11093452.0000 - mse: 11093452.0000 - mae: 2356.4175 - val_loss: 10994063.0000 - val_mse: 10994063.0000 - val_mae: 2348.0042\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 17s 113ms/step - loss: 12168364.0000 - mse: 12168364.0000 - mae: 2529.7102 - val_loss: 36613656.0000 - val_mse: 36613656.0000 - val_mae: 4916.4492\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 19297190.0000 - mse: 19297190.0000 - mae: 3219.4888 - val_loss: 10604225.0000 - val_mse: 10604225.0000 - val_mae: 2313.9341\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 6463158.5000 - mse: 6463158.5000 - mae: 1605.3076 - val_loss: 6225105.0000 - val_mse: 6225105.0000 - val_mae: 1643.9133\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 4855172.0000 - mse: 4855172.0000 - mae: 1415.5054 - val_loss: 5033243.5000 - val_mse: 5033243.5000 - val_mae: 1509.8792\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 4231112.5000 - mse: 4231112.5000 - mae: 1267.2870 - val_loss: 3970735.2500 - val_mse: 3970735.2500 - val_mae: 1197.5010\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 3640897.5000 - mse: 3640897.5000 - mae: 1197.8788 - val_loss: 3680370.0000 - val_mse: 3680370.0000 - val_mae: 1210.1877\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 2759234.0000 - mse: 2759234.0000 - mae: 1065.9863 - val_loss: 3463948.7500 - val_mse: 3463948.7500 - val_mae: 1168.1553\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 3157593.5000 - mse: 3157593.5000 - mae: 1154.5114 - val_loss: 4579517.0000 - val_mse: 4579517.0000 - val_mae: 1284.4810\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 3020375.0000 - mse: 3020375.0000 - mae: 1130.1238 - val_loss: 2516062.7500 - val_mse: 2516062.7500 - val_mae: 1014.2690\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 1721547.0000 - mse: 1721547.0000 - mae: 839.5706 - val_loss: 1419568.2500 - val_mse: 1419568.2500 - val_mae: 724.8972\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 1286829.8750 - mse: 1286829.8750 - mae: 728.3442 - val_loss: 1168025.6250 - val_mse: 1168025.6250 - val_mae: 689.5316\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 767761.8125 - mse: 767761.8125 - mae: 587.7646 - val_loss: 788892.8750 - val_mse: 788892.8750 - val_mae: 569.8605\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 13s 89ms/step - loss: 546718.9375 - mse: 546718.9375 - mae: 455.2878 - val_loss: 592070.2500 - val_mse: 592070.2500 - val_mae: 515.3499\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 581766.3125 - mse: 581766.3125 - mae: 479.8391 - val_loss: 535105.7500 - val_mse: 535105.7500 - val_mae: 471.6530\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 453537.9062 - mse: 453537.9062 - mae: 423.0237 - val_loss: 680748.9375 - val_mse: 680748.9375 - val_mae: 564.7845\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 470851.3750 - mse: 470851.3750 - mae: 440.3572 - val_loss: 634316.2500 - val_mse: 634316.2500 - val_mae: 490.8359\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 370223.7500 - mse: 370223.7500 - mae: 393.5058 - val_loss: 374403.8750 - val_mse: 374403.8750 - val_mae: 381.5214\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 337843.8125 - mse: 337843.8125 - mae: 374.7227 - val_loss: 333745.8438 - val_mse: 333745.8438 - val_mae: 358.5927\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 317003.4062 - mse: 317003.4062 - mae: 363.7688 - val_loss: 444172.1875 - val_mse: 444172.1875 - val_mae: 464.4346\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 423056.3438 - mse: 423056.3438 - mae: 428.7892 - val_loss: 427746.0625 - val_mse: 427746.0625 - val_mae: 437.5629\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 377058.9688 - mse: 377058.9688 - mae: 410.1010 - val_loss: 409652.6875 - val_mse: 409652.6875 - val_mae: 427.7499\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 18s 122ms/step - loss: 377299.9062 - mse: 377299.9062 - mae: 385.5164 - val_loss: 375153.3125 - val_mse: 375153.3125 - val_mae: 386.1922\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 30s 209ms/step - loss: 252740.0000 - mse: 252740.0000 - mae: 326.8339 - val_loss: 268589.1562 - val_mse: 268589.1562 - val_mae: 330.3336\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 285479.7188 - mse: 285479.7188 - mae: 335.8950 - val_loss: 337879.1250 - val_mse: 337879.1250 - val_mae: 350.5986\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 269751.0938 - mse: 269751.0938 - mae: 330.4676 - val_loss: 286399.0938 - val_mse: 286399.0938 - val_mae: 349.7172\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 361860.5625 - mse: 361860.5625 - mae: 401.6769 - val_loss: 481035.6562 - val_mse: 481035.6562 - val_mae: 509.8655\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 413076.1250 - mse: 413076.1250 - mae: 441.5453 - val_loss: 591705.6875 - val_mse: 591705.6875 - val_mae: 567.9930\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 327794.2188 - mse: 327794.2188 - mae: 389.6449 - val_loss: 376650.1875 - val_mse: 376650.1875 - val_mae: 423.2760\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 280829.8125 - mse: 280829.8125 - mae: 356.9104 - val_loss: 259077.7031 - val_mse: 259077.7031 - val_mae: 342.9713\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 363620.5312 - mse: 363620.5312 - mae: 387.6678 - val_loss: 416687.4375 - val_mse: 416687.4375 - val_mae: 404.4308\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 404427.1562 - mse: 404427.1562 - mae: 400.8005 - val_loss: 412420.5312 - val_mse: 412420.5312 - val_mae: 379.3618\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 260966.6250 - mse: 260966.6250 - mae: 335.5298 - val_loss: 296361.3125 - val_mse: 296361.3125 - val_mae: 350.8510\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 17s 116ms/step - loss: 221504.8750 - mse: 221504.8750 - mae: 306.2101 - val_loss: 204822.3906 - val_mse: 204822.3906 - val_mae: 300.2108\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 18s 120ms/step - loss: 265825.9062 - mse: 265825.9062 - mae: 339.8245 - val_loss: 219827.9219 - val_mse: 219827.9219 - val_mae: 308.1378\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 18s 125ms/step - loss: 241576.9219 - mse: 241576.9219 - mae: 327.8011 - val_loss: 212983.8750 - val_mse: 212983.8750 - val_mae: 275.0212\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 202548.0625 - mse: 202548.0625 - mae: 296.1510 - val_loss: 188315.6094 - val_mse: 188315.6094 - val_mae: 286.4062\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 226601.7188 - mse: 226601.7188 - mae: 297.6149 - val_loss: 333960.5938 - val_mse: 333960.5938 - val_mae: 359.0016\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 255181.3281 - mse: 255181.3281 - mae: 318.5587 - val_loss: 184149.9062 - val_mse: 184149.9062 - val_mae: 287.0147\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 288312.3125 - mse: 288312.3125 - mae: 343.0701 - val_loss: 522039.7500 - val_mse: 522039.7500 - val_mae: 495.1969\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 17s 113ms/step - loss: 378748.2812 - mse: 378748.2812 - mae: 421.5580 - val_loss: 326122.0938 - val_mse: 326122.0938 - val_mae: 376.7288\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 309628.4688 - mse: 309628.4688 - mae: 364.8542 - val_loss: 420581.8125 - val_mse: 420581.8125 - val_mae: 404.1370\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 16s 106ms/step - loss: 431124.7500 - mse: 431124.7500 - mae: 415.7669 - val_loss: 629160.3125 - val_mse: 629160.3125 - val_mae: 469.7264\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 402554.4375 - mse: 402554.4375 - mae: 414.9110 - val_loss: 377633.6875 - val_mse: 377633.6875 - val_mae: 390.2699\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 348547.2188 - mse: 348547.2188 - mae: 383.6855 - val_loss: 336212.9062 - val_mse: 336212.9062 - val_mae: 380.5872\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 314103.8125 - mse: 314103.8125 - mae: 363.9422 - val_loss: 510647.3750 - val_mse: 510647.3750 - val_mae: 411.3498\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 268270.1875 - mse: 268270.1875 - mae: 341.8445 - val_loss: 249132.5469 - val_mse: 249132.5469 - val_mae: 323.6935\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 270977.7812 - mse: 270977.7812 - mae: 341.4373 - val_loss: 252167.7812 - val_mse: 252167.7812 - val_mae: 320.5235\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 210555.8125 - mse: 210555.8125 - mae: 306.7786 - val_loss: 184064.8750 - val_mse: 184064.8750 - val_mae: 293.1569\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 198886.5469 - mse: 198886.5469 - mae: 300.6573 - val_loss: 137017.7344 - val_mse: 137017.7344 - val_mae: 251.5951\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 17s 116ms/step - loss: 140949.7188 - mse: 140949.7188 - mae: 244.3856 - val_loss: 198666.0625 - val_mse: 198666.0625 - val_mae: 285.2204\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 17s 120ms/step - loss: 240387.4062 - mse: 240387.4062 - mae: 317.1501 - val_loss: 204999.1094 - val_mse: 204999.1094 - val_mae: 293.0097\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 17s 120ms/step - loss: 230051.1406 - mse: 230051.1406 - mae: 302.4931 - val_loss: 211007.1875 - val_mse: 211007.1875 - val_mae: 296.9755\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 18s 120ms/step - loss: 176713.5469 - mse: 176713.5469 - mae: 277.6999 - val_loss: 253653.9844 - val_mse: 253653.9844 - val_mae: 311.9010\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 17s 118ms/step - loss: 265586.4062 - mse: 265586.4062 - mae: 332.8368 - val_loss: 274016.5000 - val_mse: 274016.5000 - val_mae: 337.0226\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 18s 124ms/step - loss: 270676.0312 - mse: 270676.0312 - mae: 341.5504 - val_loss: 245919.9531 - val_mse: 245919.9531 - val_mae: 338.6161\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 18s 126ms/step - loss: 271294.3125 - mse: 271294.3125 - mae: 349.7337 - val_loss: 188546.8906 - val_mse: 188546.8906 - val_mae: 286.3152\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 18s 120ms/step - loss: 310495.5938 - mse: 310495.5938 - mae: 357.1170 - val_loss: 333270.3750 - val_mse: 333270.3750 - val_mae: 405.1456\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 17s 120ms/step - loss: 328148.7188 - mse: 328148.7188 - mae: 364.5176 - val_loss: 240060.0625 - val_mse: 240060.0625 - val_mae: 331.1835\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 18s 122ms/step - loss: 264127.9062 - mse: 264127.9062 - mae: 338.2906 - val_loss: 207798.4062 - val_mse: 207798.4062 - val_mae: 290.9209\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 18s 124ms/step - loss: 208208.6406 - mse: 208208.6406 - mae: 291.3769 - val_loss: 187416.5312 - val_mse: 187416.5312 - val_mae: 283.2345\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 18s 127ms/step - loss: 141957.5000 - mse: 141957.5000 - mae: 250.2846 - val_loss: 115145.7109 - val_mse: 115145.7109 - val_mae: 219.9785\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 16s 110ms/step - loss: 141271.0781 - mse: 141271.0781 - mae: 254.2482 - val_loss: 158208.6719 - val_mse: 158208.6719 - val_mae: 262.4168\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 17s 118ms/step - loss: 148225.3281 - mse: 148225.3281 - mae: 254.5651 - val_loss: 201736.7969 - val_mse: 201736.7969 - val_mae: 323.1371\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 17s 116ms/step - loss: 7900250.0000 - mse: 7900250.0000 - mae: 1103.6208 - val_loss: 990781.8125 - val_mse: 990781.8125 - val_mae: 700.7468\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 499068.8125 - mse: 499068.8125 - mae: 485.9612 - val_loss: 402366.2500 - val_mse: 402366.2500 - val_mae: 391.7962\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 305544.9375 - mse: 305544.9375 - mae: 370.5880 - val_loss: 504248.8125 - val_mse: 504248.8125 - val_mae: 489.6226\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 235831.7656 - mse: 235831.7656 - mae: 324.2482 - val_loss: 181582.5938 - val_mse: 181582.5938 - val_mae: 287.4478\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 156093.7344 - mse: 156093.7344 - mae: 267.7298 - val_loss: 115215.4766 - val_mse: 115215.4766 - val_mae: 229.5217\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 17s 117ms/step - loss: 104986.8984 - mse: 104986.8984 - mae: 220.7062 - val_loss: 101601.5781 - val_mse: 101601.5781 - val_mae: 215.7335\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 80029.3281 - mse: 80029.3281 - mae: 197.0077 - val_loss: 78622.0469 - val_mse: 78622.0469 - val_mae: 197.4304\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 18s 120ms/step - loss: 70446.4297 - mse: 70446.4297 - mae: 190.8638 - val_loss: 81201.1328 - val_mse: 81201.1328 - val_mae: 208.1763\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 18s 121ms/step - loss: 68009.1250 - mse: 68009.1250 - mae: 192.2046 - val_loss: 62635.8828 - val_mse: 62635.8828 - val_mae: 178.2813\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 62093.3398 - mse: 62093.3398 - mae: 182.5536 - val_loss: 114188.2656 - val_mse: 114188.2656 - val_mae: 262.5019\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 88835.5391 - mse: 88835.5391 - mae: 221.4480 - val_loss: 92445.3438 - val_mse: 92445.3438 - val_mae: 219.9051\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 88449.9062 - mse: 88449.9062 - mae: 208.0340 - val_loss: 65719.4531 - val_mse: 65719.4531 - val_mae: 182.1630\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 51604.7539 - mse: 51604.7539 - mae: 161.2165 - val_loss: 56416.2031 - val_mse: 56416.2031 - val_mae: 158.0456\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 39139.6836 - mse: 39139.6836 - mae: 137.1124 - val_loss: 35739.2852 - val_mse: 35739.2852 - val_mae: 130.1252\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 18s 123ms/step - loss: 31481.8145 - mse: 31481.8145 - mae: 124.8868 - val_loss: 32722.7754 - val_mse: 32722.7754 - val_mae: 117.5162\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 27132.6602 - mse: 27132.6602 - mae: 115.2353 - val_loss: 22078.8945 - val_mse: 22078.8945 - val_mae: 94.5677\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 22040.6484 - mse: 22040.6484 - mae: 104.4853 - val_loss: 24899.1660 - val_mse: 24899.1660 - val_mae: 107.7888\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 26782.1680 - mse: 26782.1680 - mae: 115.9077 - val_loss: 38157.8164 - val_mse: 38157.8164 - val_mae: 129.8667\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 16s 106ms/step - loss: 28557.8398 - mse: 28557.8398 - mae: 116.9378 - val_loss: 32265.0020 - val_mse: 32265.0020 - val_mae: 120.5404\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 22911.0723 - mse: 22911.0723 - mae: 105.8575 - val_loss: 22482.7578 - val_mse: 22482.7578 - val_mae: 99.6852\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 20364.0195 - mse: 20364.0195 - mae: 99.3196 - val_loss: 19832.3828 - val_mse: 19832.3828 - val_mae: 91.3829\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 17s 118ms/step - loss: 106868.8594 - mse: 106868.8594 - mae: 202.4900 - val_loss: 307382.7812 - val_mse: 307382.7812 - val_mae: 350.1985\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 315256.3750 - mse: 315256.3750 - mae: 377.7894 - val_loss: 287485.5625 - val_mse: 287485.5625 - val_mae: 356.2007\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 18s 121ms/step - loss: 261924.4844 - mse: 261924.4844 - mae: 356.1563 - val_loss: 310786.2500 - val_mse: 310786.2500 - val_mae: 328.9419\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 17s 116ms/step - loss: 245524.9844 - mse: 245524.9844 - mae: 325.6639 - val_loss: 247186.3906 - val_mse: 247186.3906 - val_mae: 311.5355\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 214230.3438 - mse: 214230.3438 - mae: 314.0407 - val_loss: 219373.4688 - val_mse: 219373.4688 - val_mae: 325.7719\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 141346.5938 - mse: 141346.5938 - mae: 251.4250 - val_loss: 158165.4844 - val_mse: 158165.4844 - val_mae: 238.3241\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 142593.4688 - mse: 142593.4688 - mae: 243.8593 - val_loss: 148793.0156 - val_mse: 148793.0156 - val_mae: 240.6224\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 132766.4531 - mse: 132766.4531 - mae: 228.8230 - val_loss: 200701.7500 - val_mse: 200701.7500 - val_mae: 298.7539\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 162019.6094 - mse: 162019.6094 - mae: 265.2440 - val_loss: 203796.2812 - val_mse: 203796.2812 - val_mae: 311.0088\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 135854.7656 - mse: 135854.7656 - mae: 256.1696 - val_loss: 234815.2500 - val_mse: 234815.2500 - val_mae: 369.1891\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 113867.5938 - mse: 113867.5938 - mae: 237.7696 - val_loss: 96090.2344 - val_mse: 96090.2344 - val_mae: 200.8900\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 120967.7109 - mse: 120967.7109 - mae: 238.3258 - val_loss: 126609.5078 - val_mse: 126609.5078 - val_mae: 255.7503\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 242948.6094 - mse: 242948.6094 - mae: 330.7639 - val_loss: 378562.8750 - val_mse: 378562.8750 - val_mae: 352.4683\n",
      "MAE LSTM fold n°1 = 719.91\n",
      "🏋🏽‍♂️ improvement over baseline: -108.44 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°2 = 345.88\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 18s 106ms/step - loss: 54830644.0000 - mse: 54830644.0000 - mae: 6726.4580 - val_loss: 48465852.0000 - val_mse: 48465852.0000 - val_mae: 6135.1060\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 32710786.0000 - mse: 32710786.0000 - mae: 4758.9312 - val_loss: 22520710.0000 - val_mse: 22520710.0000 - val_mae: 3484.1174\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 13965418.0000 - mse: 13965418.0000 - mae: 2699.4299 - val_loss: 11858201.0000 - val_mse: 11858201.0000 - val_mae: 2401.2979\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 9724272.0000 - mse: 9724272.0000 - mae: 2197.3652 - val_loss: 10871075.0000 - val_mse: 10871075.0000 - val_mae: 2376.3684\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 18982974.0000 - mse: 18982974.0000 - mae: 2944.6814 - val_loss: 7606371.5000 - val_mse: 7606371.5000 - val_mae: 1944.8098\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 6286936.5000 - mse: 6286936.5000 - mae: 1772.7021 - val_loss: 6299171.5000 - val_mse: 6299171.5000 - val_mae: 1695.5604\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 5820627.0000 - mse: 5820627.0000 - mae: 1629.0056 - val_loss: 4662186.5000 - val_mse: 4662186.5000 - val_mae: 1287.5873\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 2349988.5000 - mse: 2349988.5000 - mae: 956.3839 - val_loss: 1589374.7500 - val_mse: 1589374.7500 - val_mae: 776.4457\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 914397.8750 - mse: 914397.8750 - mae: 593.6005 - val_loss: 1001445.3750 - val_mse: 1001445.3750 - val_mae: 633.1946\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 16s 110ms/step - loss: 549551.6875 - mse: 549551.6875 - mae: 454.9664 - val_loss: 509111.8750 - val_mse: 509111.8750 - val_mae: 414.6324\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 388905.8750 - mse: 388905.8750 - mae: 370.3578 - val_loss: 383223.9375 - val_mse: 383223.9375 - val_mae: 361.7950\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 343083.3438 - mse: 343083.3438 - mae: 352.8012 - val_loss: 361076.2500 - val_mse: 361076.2500 - val_mae: 345.9893\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 16s 106ms/step - loss: 326931.2188 - mse: 326931.2188 - mae: 341.6724 - val_loss: 442508.0000 - val_mse: 442508.0000 - val_mae: 378.2538\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 290157.0000 - mse: 290157.0000 - mae: 311.0748 - val_loss: 372959.2500 - val_mse: 372959.2500 - val_mae: 334.2145\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 271412.8750 - mse: 271412.8750 - mae: 326.3004 - val_loss: 444452.9375 - val_mse: 444452.9375 - val_mae: 361.0107\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 16s 111ms/step - loss: 301353.2188 - mse: 301353.2188 - mae: 337.7777 - val_loss: 311532.0938 - val_mse: 311532.0938 - val_mae: 324.2519\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 16s 108ms/step - loss: 307377.1250 - mse: 307377.1250 - mae: 347.7101 - val_loss: 294526.5625 - val_mse: 294526.5625 - val_mae: 316.3108\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 17s 119ms/step - loss: 289105.8438 - mse: 289105.8438 - mae: 345.4415 - val_loss: 324625.2188 - val_mse: 324625.2188 - val_mae: 356.1125\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 17s 116ms/step - loss: 262273.6875 - mse: 262273.6875 - mae: 328.2576 - val_loss: 194812.2031 - val_mse: 194812.2031 - val_mae: 278.5440\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 236663.6562 - mse: 236663.6562 - mae: 319.0729 - val_loss: 188512.6719 - val_mse: 188512.6719 - val_mae: 297.0926\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 233901.0156 - mse: 233901.0156 - mae: 315.1678 - val_loss: 362012.4375 - val_mse: 362012.4375 - val_mae: 361.5635\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 297477.5000 - mse: 297477.5000 - mae: 351.1542 - val_loss: 210863.0312 - val_mse: 210863.0312 - val_mae: 297.6668\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 214965.3906 - mse: 214965.3906 - mae: 296.6353 - val_loss: 159587.1719 - val_mse: 159587.1719 - val_mae: 265.0450\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 17s 113ms/step - loss: 216390.0469 - mse: 216390.0469 - mae: 308.9284 - val_loss: 188082.1406 - val_mse: 188082.1406 - val_mae: 293.5532\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 16s 106ms/step - loss: 204988.4688 - mse: 204988.4688 - mae: 287.7537 - val_loss: 164826.0469 - val_mse: 164826.0469 - val_mae: 266.4322\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 176050.3750 - mse: 176050.3750 - mae: 278.9231 - val_loss: 191566.0625 - val_mse: 191566.0625 - val_mae: 276.9278\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 17s 114ms/step - loss: 228367.5312 - mse: 228367.5312 - mae: 305.8423 - val_loss: 219993.3281 - val_mse: 219993.3281 - val_mae: 310.6945\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 243280.9219 - mse: 243280.9219 - mae: 314.1596 - val_loss: 289920.7500 - val_mse: 289920.7500 - val_mae: 325.4944\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 705s 5s/step - loss: 238853.3438 - mse: 238853.3438 - mae: 308.8949 - val_loss: 224142.3906 - val_mse: 224142.3906 - val_mae: 327.5000\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 17s 115ms/step - loss: 193882.7656 - mse: 193882.7656 - mae: 271.0754 - val_loss: 147350.9688 - val_mse: 147350.9688 - val_mae: 235.8811\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 15s 99ms/step - loss: 161011.9375 - mse: 161011.9375 - mae: 262.0741 - val_loss: 162932.9062 - val_mse: 162932.9062 - val_mae: 277.6552\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 97s 665ms/step - loss: 158264.4531 - mse: 158264.4531 - mae: 252.7405 - val_loss: 151466.3750 - val_mse: 151466.3750 - val_mae: 250.5735\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 237560.9219 - mse: 237560.9219 - mae: 310.4231 - val_loss: 206117.1250 - val_mse: 206117.1250 - val_mae: 294.2563\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 336925.4688 - mse: 336925.4688 - mae: 348.6336 - val_loss: 789487.7500 - val_mse: 789487.7500 - val_mae: 583.0858\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 400610.1562 - mse: 400610.1562 - mae: 403.2700 - val_loss: 294123.5625 - val_mse: 294123.5625 - val_mae: 324.9462\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 243165.2969 - mse: 243165.2969 - mae: 307.0091 - val_loss: 212375.2344 - val_mse: 212375.2344 - val_mae: 302.1954\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 234094.0312 - mse: 234094.0312 - mae: 295.3057 - val_loss: 186703.8125 - val_mse: 186703.8125 - val_mae: 285.6650\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 175610.9062 - mse: 175610.9062 - mae: 267.7146 - val_loss: 191608.8594 - val_mse: 191608.8594 - val_mae: 273.2890\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 172071.3750 - mse: 172071.3750 - mae: 268.8640 - val_loss: 119731.3672 - val_mse: 119731.3672 - val_mae: 225.5892\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 143876.7344 - mse: 143876.7344 - mae: 240.6872 - val_loss: 119974.9922 - val_mse: 119974.9922 - val_mae: 223.1862\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 124333.6484 - mse: 124333.6484 - mae: 229.5333 - val_loss: 91223.7500 - val_mse: 91223.7500 - val_mae: 207.3552\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 105701.4531 - mse: 105701.4531 - mae: 206.8477 - val_loss: 112625.3125 - val_mse: 112625.3125 - val_mae: 215.3311\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 105329.6641 - mse: 105329.6641 - mae: 220.1255 - val_loss: 96211.3594 - val_mse: 96211.3594 - val_mae: 207.4802\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 107310.9297 - mse: 107310.9297 - mae: 214.8754 - val_loss: 141774.6875 - val_mse: 141774.6875 - val_mae: 244.0119\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 120849.1875 - mse: 120849.1875 - mae: 226.6508 - val_loss: 78634.9922 - val_mse: 78634.9922 - val_mae: 176.1612\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 90264.9531 - mse: 90264.9531 - mae: 186.1803 - val_loss: 66222.6797 - val_mse: 66222.6797 - val_mae: 167.9991\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 71013.2500 - mse: 71013.2500 - mae: 176.6240 - val_loss: 65386.9648 - val_mse: 65386.9648 - val_mae: 162.3831\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 71722.9453 - mse: 71722.9453 - mae: 167.4610 - val_loss: 85792.2656 - val_mse: 85792.2656 - val_mae: 173.0366\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 90951.1094 - mse: 90951.1094 - mae: 188.8804 - val_loss: 179530.3906 - val_mse: 179530.3906 - val_mae: 205.9647\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 225487.8438 - mse: 225487.8438 - mae: 307.4607 - val_loss: 367251.3125 - val_mse: 367251.3125 - val_mae: 366.3667\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 234513.2812 - mse: 234513.2812 - mae: 294.1143 - val_loss: 209736.5156 - val_mse: 209736.5156 - val_mae: 278.3287\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 289651.6562 - mse: 289651.6562 - mae: 328.3821 - val_loss: 331444.4375 - val_mse: 331444.4375 - val_mae: 379.8600\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 421137.8125 - mse: 421137.8125 - mae: 405.9183 - val_loss: 381545.6250 - val_mse: 381545.6250 - val_mae: 380.2108\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 404531.6875 - mse: 404531.6875 - mae: 403.6347 - val_loss: 444856.3750 - val_mse: 444856.3750 - val_mae: 406.8070\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 15s 103ms/step - loss: 383826.2812 - mse: 383826.2812 - mae: 407.8062 - val_loss: 309867.7812 - val_mse: 309867.7812 - val_mae: 336.0178\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 293210.0312 - mse: 293210.0312 - mae: 341.1620 - val_loss: 366872.9062 - val_mse: 366872.9062 - val_mae: 368.6215\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 15s 105ms/step - loss: 261405.1406 - mse: 261405.1406 - mae: 332.3922 - val_loss: 199185.8125 - val_mse: 199185.8125 - val_mae: 290.4508\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 239190.1406 - mse: 239190.1406 - mae: 314.8672 - val_loss: 216575.0781 - val_mse: 216575.0781 - val_mae: 308.7328\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 228090.0000 - mse: 228090.0000 - mae: 313.0453 - val_loss: 236600.7031 - val_mse: 236600.7031 - val_mae: 329.6865\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 187053.3125 - mse: 187053.3125 - mae: 275.8792 - val_loss: 177071.8125 - val_mse: 177071.8125 - val_mae: 270.4077\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 158704.2188 - mse: 158704.2188 - mae: 250.0655 - val_loss: 179379.9688 - val_mse: 179379.9688 - val_mae: 288.8514\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 15s 106ms/step - loss: 157776.3594 - mse: 157776.3594 - mae: 257.6363 - val_loss: 187069.4062 - val_mse: 187069.4062 - val_mae: 308.2855\n",
      "MAE LSTM fold n°2 = 466.71\n",
      "🏋🏽‍♂️ improvement over baseline: -34.93 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°3 = 310.44\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 16s 93ms/step - loss: 53080880.0000 - mse: 53080880.0000 - mae: 6585.9136 - val_loss: 41996264.0000 - val_mse: 41996264.0000 - val_mae: 5583.0049\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 23523624.0000 - mse: 23523624.0000 - mae: 3774.1809 - val_loss: 14002861.0000 - val_mse: 14002861.0000 - val_mae: 2695.2793\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 10146803.0000 - mse: 10146803.0000 - mae: 2247.0442 - val_loss: 10869162.0000 - val_mse: 10869162.0000 - val_mae: 2377.3718\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 9513840.0000 - mse: 9513840.0000 - mae: 2245.6211 - val_loss: 10833304.0000 - val_mse: 10833304.0000 - val_mae: 2401.2786\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 11886431.0000 - mse: 11886431.0000 - mae: 2570.3162 - val_loss: 14907971.0000 - val_mse: 14907971.0000 - val_mae: 3072.8198\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 13881905.0000 - mse: 13881905.0000 - mae: 3029.0498 - val_loss: 14770938.0000 - val_mse: 14770938.0000 - val_mae: 3132.0647\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 21952276.0000 - mse: 21952276.0000 - mae: 3780.4255 - val_loss: 30721580.0000 - val_mse: 30721580.0000 - val_mae: 4410.8164\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 38552612.0000 - mse: 38552612.0000 - mae: 5253.1987 - val_loss: 12069349.0000 - val_mse: 12069349.0000 - val_mae: 2370.9116\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 7186726.0000 - mse: 7186726.0000 - mae: 1683.5543 - val_loss: 8091363.0000 - val_mse: 8091363.0000 - val_mae: 1799.5583\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 7147949.0000 - mse: 7147949.0000 - mae: 1805.4415 - val_loss: 7924178.5000 - val_mse: 7924178.5000 - val_mae: 1845.6763\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 6829222.0000 - mse: 6829222.0000 - mae: 1703.9722 - val_loss: 6529097.5000 - val_mse: 6529097.5000 - val_mae: 1614.9392\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 3148085.2500 - mse: 3148085.2500 - mae: 1033.7219 - val_loss: 2594815.7500 - val_mse: 2594815.7500 - val_mae: 906.0281\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 1518437.0000 - mse: 1518437.0000 - mae: 737.1130 - val_loss: 1218658.7500 - val_mse: 1218658.7500 - val_mae: 575.6959\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 772190.3750 - mse: 772190.3750 - mae: 514.9028 - val_loss: 829050.1875 - val_mse: 829050.1875 - val_mae: 525.0225\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 1024667.6875 - mse: 1024667.6875 - mae: 662.3906 - val_loss: 893962.0000 - val_mse: 893962.0000 - val_mae: 614.6213\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 751973.6250 - mse: 751973.6250 - mae: 581.7327 - val_loss: 876699.1250 - val_mse: 876699.1250 - val_mae: 686.4809\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 502758.2812 - mse: 502758.2812 - mae: 471.6581 - val_loss: 574773.0625 - val_mse: 574773.0625 - val_mae: 488.7018\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 416353.2812 - mse: 416353.2812 - mae: 416.9980 - val_loss: 435051.2500 - val_mse: 435051.2500 - val_mae: 407.3295\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 334517.1875 - mse: 334517.1875 - mae: 361.5205 - val_loss: 356886.6875 - val_mse: 356886.6875 - val_mae: 337.9025\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 315191.8125 - mse: 315191.8125 - mae: 352.4238 - val_loss: 354131.6250 - val_mse: 354131.6250 - val_mae: 354.6568\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 376383.0000 - mse: 376383.0000 - mae: 398.7496 - val_loss: 395721.2812 - val_mse: 395721.2812 - val_mae: 398.8825\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 337987.9375 - mse: 337987.9375 - mae: 376.6929 - val_loss: 366758.3750 - val_mse: 366758.3750 - val_mae: 373.2096\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 405113.3750 - mse: 405113.3750 - mae: 410.7512 - val_loss: 456049.3750 - val_mse: 456049.3750 - val_mae: 461.7792\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 360058.9375 - mse: 360058.9375 - mae: 388.2739 - val_loss: 373830.4688 - val_mse: 373830.4688 - val_mae: 359.1643\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 385638.4375 - mse: 385638.4375 - mae: 399.8386 - val_loss: 334129.3750 - val_mse: 334129.3750 - val_mae: 381.2320\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 277402.8125 - mse: 277402.8125 - mae: 340.6264 - val_loss: 371707.3750 - val_mse: 371707.3750 - val_mae: 376.3562\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 309611.6875 - mse: 309611.6875 - mae: 347.8256 - val_loss: 242770.1094 - val_mse: 242770.1094 - val_mae: 305.8343\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 222594.1562 - mse: 222594.1562 - mae: 294.0736 - val_loss: 210438.2031 - val_mse: 210438.2031 - val_mae: 273.4453\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 246562.7344 - mse: 246562.7344 - mae: 302.2114 - val_loss: 227665.4531 - val_mse: 227665.4531 - val_mae: 286.9785\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 209996.1875 - mse: 209996.1875 - mae: 280.2131 - val_loss: 203228.6406 - val_mse: 203228.6406 - val_mae: 267.3134\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 214988.4219 - mse: 214988.4219 - mae: 282.6469 - val_loss: 233181.7812 - val_mse: 233181.7812 - val_mae: 300.2176\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 217283.0000 - mse: 217283.0000 - mae: 283.4477 - val_loss: 236758.2812 - val_mse: 236758.2812 - val_mae: 290.4370\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 304770.8750 - mse: 304770.8750 - mae: 347.4866 - val_loss: 279192.8750 - val_mse: 279192.8750 - val_mae: 324.8884\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 295979.6875 - mse: 295979.6875 - mae: 352.3708 - val_loss: 378547.0938 - val_mse: 378547.0938 - val_mae: 439.8716\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 338970.4062 - mse: 338970.4062 - mae: 386.4609 - val_loss: 286757.0312 - val_mse: 286757.0312 - val_mae: 340.7774\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 314615.5312 - mse: 314615.5312 - mae: 362.1636 - val_loss: 297110.2188 - val_mse: 297110.2188 - val_mae: 338.2730\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 299973.7188 - mse: 299973.7188 - mae: 355.7947 - val_loss: 350644.0000 - val_mse: 350644.0000 - val_mae: 349.6592\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 281113.5625 - mse: 281113.5625 - mae: 325.6416 - val_loss: 371438.4375 - val_mse: 371438.4375 - val_mae: 407.1370\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 15s 101ms/step - loss: 305051.2500 - mse: 305051.2500 - mae: 347.0411 - val_loss: 296771.5312 - val_mse: 296771.5312 - val_mae: 330.5638\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 297764.0312 - mse: 297764.0312 - mae: 338.2567 - val_loss: 267995.0938 - val_mse: 267995.0938 - val_mae: 321.3264\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 278346.0000 - mse: 278346.0000 - mae: 324.4638 - val_loss: 436538.6875 - val_mse: 436538.6875 - val_mae: 435.5989\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 304470.0625 - mse: 304470.0625 - mae: 339.2316 - val_loss: 284839.0938 - val_mse: 284839.0938 - val_mae: 320.6985\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 265850.9375 - mse: 265850.9375 - mae: 322.9778 - val_loss: 306902.5000 - val_mse: 306902.5000 - val_mae: 358.2606\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 279816.5000 - mse: 279816.5000 - mae: 321.5050 - val_loss: 264740.3750 - val_mse: 264740.3750 - val_mae: 302.6091\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 474619.5000 - mse: 474619.5000 - mae: 433.2902 - val_loss: 774039.4375 - val_mse: 774039.4375 - val_mae: 613.0060\n",
      "MAE LSTM fold n°3 = 473.61\n",
      "🏋🏽‍♂️ improvement over baseline: -52.56 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°4 = 291.89\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 17s 96ms/step - loss: 51614800.0000 - mse: 51614800.0000 - mae: 6467.7334 - val_loss: 39080040.0000 - val_mse: 39080040.0000 - val_mae: 5315.4224\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 21099892.0000 - mse: 21099892.0000 - mae: 3503.5149 - val_loss: 12771524.0000 - val_mse: 12771524.0000 - val_mae: 2536.3379\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 9834916.0000 - mse: 9834916.0000 - mae: 2217.6479 - val_loss: 10846620.0000 - val_mse: 10846620.0000 - val_mae: 2389.2585\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 21434700.0000 - mse: 21434700.0000 - mae: 3747.3413 - val_loss: 25855228.0000 - val_mse: 25855228.0000 - val_mae: 4275.4341\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 30427602.0000 - mse: 30427602.0000 - mae: 4740.2588 - val_loss: 62905252.0000 - val_mse: 62905252.0000 - val_mae: 7216.5723\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 59882700.0000 - mse: 59882700.0000 - mae: 7097.6528 - val_loss: 62871832.0000 - val_mse: 62871832.0000 - val_mae: 7214.2578\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 59852320.0000 - mse: 59852320.0000 - mae: 7095.5088 - val_loss: 62842952.0000 - val_mse: 62842952.0000 - val_mae: 7212.2568\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 59825160.0000 - mse: 59825160.0000 - mae: 7093.5947 - val_loss: 62816420.0000 - val_mse: 62816420.0000 - val_mae: 7210.4160\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 59799796.0000 - mse: 59799796.0000 - mae: 7091.8071 - val_loss: 62791328.0000 - val_mse: 62791328.0000 - val_mae: 7208.6758\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 59775600.0000 - mse: 59775600.0000 - mae: 7090.1001 - val_loss: 62767156.0000 - val_mse: 62767156.0000 - val_mae: 7206.9995\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 59752168.0000 - mse: 59752168.0000 - mae: 7088.4473 - val_loss: 62743664.0000 - val_mse: 62743664.0000 - val_mae: 7205.3687\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 59729296.0000 - mse: 59729296.0000 - mae: 7086.8364 - val_loss: 62720660.0000 - val_mse: 62720660.0000 - val_mae: 7203.7729\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 59706872.0000 - mse: 59706872.0000 - mae: 7085.2544 - val_loss: 62698044.0000 - val_mse: 62698044.0000 - val_mae: 7202.2021\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 59684784.0000 - mse: 59684784.0000 - mae: 7083.6953 - val_loss: 62675716.0000 - val_mse: 62675716.0000 - val_mae: 7200.6533\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 59662928.0000 - mse: 59662928.0000 - mae: 7082.1509 - val_loss: 62653644.0000 - val_mse: 62653644.0000 - val_mae: 7199.1191\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 59641312.0000 - mse: 59641312.0000 - mae: 7080.6270 - val_loss: 62631756.0000 - val_mse: 62631756.0000 - val_mae: 7197.5996\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 59619880.0000 - mse: 59619880.0000 - mae: 7079.1133 - val_loss: 62610032.0000 - val_mse: 62610032.0000 - val_mae: 7196.0889\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 59598568.0000 - mse: 59598568.0000 - mae: 7077.6055 - val_loss: 62588420.0000 - val_mse: 62588420.0000 - val_mae: 7194.5884\n",
      "MAE LSTM fold n°4 = 1824.47\n",
      "🏋🏽‍♂️ improvement over baseline: -525.05 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°5 = 260.99\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 16s 91ms/step - loss: 50303988.0000 - mse: 50303988.0000 - mae: 6357.0479 - val_loss: 35728992.0000 - val_mse: 35728992.0000 - val_mae: 4990.2568\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 18275306.0000 - mse: 18275306.0000 - mae: 3178.1506 - val_loss: 11705836.0000 - val_mse: 11705836.0000 - val_mae: 2378.3979\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9623505.0000 - mse: 9623505.0000 - mae: 2214.0088 - val_loss: 10835903.0000 - val_mse: 10835903.0000 - val_mae: 2398.2820\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 9513269.0000 - mse: 9513269.0000 - mae: 2252.2983 - val_loss: 10831670.0000 - val_mse: 10831670.0000 - val_mae: 2403.4578\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 9513791.0000 - mse: 9513791.0000 - mae: 2253.0913 - val_loss: 10831041.0000 - val_mse: 10831041.0000 - val_mae: 2404.3777\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9514202.0000 - mse: 9514202.0000 - mae: 2253.0190 - val_loss: 10830652.0000 - val_mse: 10830652.0000 - val_mae: 2404.9812\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9514504.0000 - mse: 9514504.0000 - mae: 2252.9507 - val_loss: 10830427.0000 - val_mse: 10830427.0000 - val_mae: 2405.3447\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 20976486.0000 - mse: 20976486.0000 - mae: 3570.6362 - val_loss: 25063456.0000 - val_mse: 25063456.0000 - val_mae: 4034.1587\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 15550217.0000 - mse: 15550217.0000 - mae: 3087.3005 - val_loss: 12000236.0000 - val_mse: 12000236.0000 - val_mae: 2321.4126\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 8050427.5000 - mse: 8050427.5000 - mae: 2059.2734 - val_loss: 8329632.5000 - val_mse: 8329632.5000 - val_mae: 2073.6333\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 6726891.5000 - mse: 6726891.5000 - mae: 1925.1458 - val_loss: 8114859.5000 - val_mse: 8114859.5000 - val_mae: 2048.9358\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 6690474.0000 - mse: 6690474.0000 - mae: 1914.4458 - val_loss: 8108721.0000 - val_mse: 8108721.0000 - val_mae: 2047.8021\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 6690274.5000 - mse: 6690274.5000 - mae: 1913.6948 - val_loss: 8108297.5000 - val_mse: 8108297.5000 - val_mae: 2047.5602\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 6688680.5000 - mse: 6688680.5000 - mae: 1910.7388 - val_loss: 7863874.0000 - val_mse: 7863874.0000 - val_mae: 1938.1165\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 5571854.0000 - mse: 5571854.0000 - mae: 1570.8103 - val_loss: 5282044.5000 - val_mse: 5282044.5000 - val_mae: 1415.4019\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 2606059.0000 - mse: 2606059.0000 - mae: 912.3979 - val_loss: 1987905.2500 - val_mse: 1987905.2500 - val_mae: 742.4140\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 1340045.2500 - mse: 1340045.2500 - mae: 589.0151 - val_loss: 1286605.1250 - val_mse: 1286605.1250 - val_mae: 496.6399\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 1061562.5000 - mse: 1061562.5000 - mae: 501.4931 - val_loss: 1067301.1250 - val_mse: 1067301.1250 - val_mae: 480.3326\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 977510.3750 - mse: 977510.3750 - mae: 494.8489 - val_loss: 933998.2500 - val_mse: 933998.2500 - val_mae: 449.8868\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 823720.4375 - mse: 823720.4375 - mae: 440.8195 - val_loss: 839449.5000 - val_mse: 839449.5000 - val_mae: 415.8451\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 775369.5000 - mse: 775369.5000 - mae: 431.9964 - val_loss: 1075151.2500 - val_mse: 1075151.2500 - val_mae: 449.0848\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 1035164.8125 - mse: 1035164.8125 - mae: 506.5195 - val_loss: 2252591.0000 - val_mse: 2252591.0000 - val_mae: 758.2236\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 1223790.5000 - mse: 1223790.5000 - mae: 544.9768 - val_loss: 1395187.8750 - val_mse: 1395187.8750 - val_mae: 558.3744\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 756844.0625 - mse: 756844.0625 - mae: 408.9072 - val_loss: 911835.6250 - val_mse: 911835.6250 - val_mae: 465.6656\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 516460.7500 - mse: 516460.7500 - mae: 350.2082 - val_loss: 616860.6875 - val_mse: 616860.6875 - val_mae: 362.7556\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 384844.2188 - mse: 384844.2188 - mae: 324.9869 - val_loss: 468359.9062 - val_mse: 468359.9062 - val_mae: 343.0211\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 386828.6875 - mse: 386828.6875 - mae: 337.4627 - val_loss: 481599.2812 - val_mse: 481599.2812 - val_mae: 382.2956\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 354077.9375 - mse: 354077.9375 - mae: 334.0895 - val_loss: 376541.3750 - val_mse: 376541.3750 - val_mae: 328.0016\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 307194.1250 - mse: 307194.1250 - mae: 329.2144 - val_loss: 404155.0625 - val_mse: 404155.0625 - val_mae: 357.6469\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 328593.1562 - mse: 328593.1562 - mae: 334.8762 - val_loss: 413261.1562 - val_mse: 413261.1562 - val_mae: 382.8453\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 326409.9688 - mse: 326409.9688 - mae: 327.6533 - val_loss: 389253.1562 - val_mse: 389253.1562 - val_mae: 339.0538\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 296090.3438 - mse: 296090.3438 - mae: 308.3438 - val_loss: 363067.2812 - val_mse: 363067.2812 - val_mae: 335.4195\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 270116.0000 - mse: 270116.0000 - mae: 293.0475 - val_loss: 284334.3125 - val_mse: 284334.3125 - val_mae: 281.0249\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 271481.9375 - mse: 271481.9375 - mae: 295.6059 - val_loss: 371900.0938 - val_mse: 371900.0938 - val_mae: 340.7318\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 279073.3750 - mse: 279073.3750 - mae: 299.1122 - val_loss: 296398.4688 - val_mse: 296398.4688 - val_mae: 301.6029\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 255034.7344 - mse: 255034.7344 - mae: 272.8709 - val_loss: 282501.2812 - val_mse: 282501.2812 - val_mae: 285.4075\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 242619.8906 - mse: 242619.8906 - mae: 260.8481 - val_loss: 278101.0312 - val_mse: 278101.0312 - val_mae: 270.9835\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 230481.0312 - mse: 230481.0312 - mae: 252.1750 - val_loss: 283478.7812 - val_mse: 283478.7812 - val_mae: 266.3541\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 233210.4844 - mse: 233210.4844 - mae: 251.1776 - val_loss: 282667.9062 - val_mse: 282667.9062 - val_mae: 295.9267\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 278916.0312 - mse: 278916.0312 - mae: 283.5465 - val_loss: 329312.4688 - val_mse: 329312.4688 - val_mae: 298.1568\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 386664.5938 - mse: 386664.5938 - mae: 377.5663 - val_loss: 400840.9688 - val_mse: 400840.9688 - val_mae: 383.7260\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 397633.4688 - mse: 397633.4688 - mae: 390.8387 - val_loss: 386653.2188 - val_mse: 386653.2188 - val_mae: 365.6325\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 470086.9375 - mse: 470086.9375 - mae: 395.3805 - val_loss: 537714.5000 - val_mse: 537714.5000 - val_mae: 449.4828\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 370228.1562 - mse: 370228.1562 - mae: 367.3170 - val_loss: 451126.6875 - val_mse: 451126.6875 - val_mae: 392.8466\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 370806.5312 - mse: 370806.5312 - mae: 355.5597 - val_loss: 310390.2812 - val_mse: 310390.2812 - val_mae: 303.9933\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 234783.3594 - mse: 234783.3594 - mae: 278.8058 - val_loss: 269994.9375 - val_mse: 269994.9375 - val_mae: 306.4194\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 234822.9688 - mse: 234822.9688 - mae: 288.8125 - val_loss: 277371.0312 - val_mse: 277371.0312 - val_mae: 280.6724\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 210477.0312 - mse: 210477.0312 - mae: 268.8588 - val_loss: 286905.5000 - val_mse: 286905.5000 - val_mae: 312.0254\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 190928.1719 - mse: 190928.1719 - mae: 244.0993 - val_loss: 204676.4688 - val_mse: 204676.4688 - val_mae: 252.4503\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 188657.5469 - mse: 188657.5469 - mae: 245.7221 - val_loss: 233149.2812 - val_mse: 233149.2812 - val_mae: 273.1106\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 188601.8750 - mse: 188601.8750 - mae: 240.2650 - val_loss: 185853.8281 - val_mse: 185853.8281 - val_mae: 223.9165\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 155866.6719 - mse: 155866.6719 - mae: 207.2657 - val_loss: 165585.0781 - val_mse: 165585.0781 - val_mae: 202.0491\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 189577.6875 - mse: 189577.6875 - mae: 247.3460 - val_loss: 201501.6250 - val_mse: 201501.6250 - val_mae: 247.9839\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 162460.7969 - mse: 162460.7969 - mae: 218.3055 - val_loss: 190097.5938 - val_mse: 190097.5938 - val_mae: 242.3929\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 197244.2344 - mse: 197244.2344 - mae: 251.7536 - val_loss: 198115.4688 - val_mse: 198115.4688 - val_mae: 270.3020\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 164958.1250 - mse: 164958.1250 - mae: 248.2097 - val_loss: 142035.5469 - val_mse: 142035.5469 - val_mae: 226.1886\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 148220.0000 - mse: 148220.0000 - mae: 215.5112 - val_loss: 190966.1250 - val_mse: 190966.1250 - val_mae: 252.7580\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 153513.0469 - mse: 153513.0469 - mae: 212.5757 - val_loss: 199829.8906 - val_mse: 199829.8906 - val_mae: 260.5299\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 147315.7188 - mse: 147315.7188 - mae: 201.3464 - val_loss: 143018.1250 - val_mse: 143018.1250 - val_mae: 217.9008\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 124385.5234 - mse: 124385.5234 - mae: 183.5064 - val_loss: 135894.1719 - val_mse: 135894.1719 - val_mae: 192.2605\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 122313.8359 - mse: 122313.8359 - mae: 174.3950 - val_loss: 118867.3984 - val_mse: 118867.3984 - val_mae: 171.5635\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 107765.9609 - mse: 107765.9609 - mae: 174.8243 - val_loss: 146188.5000 - val_mse: 146188.5000 - val_mae: 231.0019\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 150055.4375 - mse: 150055.4375 - mae: 227.4159 - val_loss: 183368.6406 - val_mse: 183368.6406 - val_mae: 277.0186\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 164004.5781 - mse: 164004.5781 - mae: 242.0802 - val_loss: 138618.5312 - val_mse: 138618.5312 - val_mae: 205.8559\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 130520.6484 - mse: 130520.6484 - mae: 210.0922 - val_loss: 201244.5781 - val_mse: 201244.5781 - val_mae: 238.7127\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 180808.3750 - mse: 180808.3750 - mae: 240.6003 - val_loss: 169726.6562 - val_mse: 169726.6562 - val_mae: 216.3551\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 163393.0000 - mse: 163393.0000 - mae: 232.0145 - val_loss: 174505.0938 - val_mse: 174505.0938 - val_mae: 242.3919\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 131723.1406 - mse: 131723.1406 - mae: 214.3264 - val_loss: 133529.6406 - val_mse: 133529.6406 - val_mae: 212.5836\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 152753.3281 - mse: 152753.3281 - mae: 228.7860 - val_loss: 271401.1250 - val_mse: 271401.1250 - val_mae: 314.2340\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 157452.8438 - mse: 157452.8438 - mae: 239.0132 - val_loss: 127024.7109 - val_mse: 127024.7109 - val_mae: 211.2553\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 158825.8125 - mse: 158825.8125 - mae: 228.0552 - val_loss: 139157.2500 - val_mse: 139157.2500 - val_mae: 219.4206\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 102122.4141 - mse: 102122.4141 - mae: 187.7186 - val_loss: 97274.2656 - val_mse: 97274.2656 - val_mae: 186.2703\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 13s 89ms/step - loss: 87926.0781 - mse: 87926.0781 - mae: 172.6587 - val_loss: 122295.7188 - val_mse: 122295.7188 - val_mae: 188.8858\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 116788.7422 - mse: 116788.7422 - mae: 191.8251 - val_loss: 118026.3672 - val_mse: 118026.3672 - val_mae: 189.2526\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 121887.1094 - mse: 121887.1094 - mae: 192.0658 - val_loss: 147965.8438 - val_mse: 147965.8438 - val_mae: 209.4643\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 189984.7031 - mse: 189984.7031 - mae: 271.7138 - val_loss: 197717.4219 - val_mse: 197717.4219 - val_mae: 289.6328\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 175729.1875 - mse: 175729.1875 - mae: 273.5164 - val_loss: 135872.1719 - val_mse: 135872.1719 - val_mae: 222.7145\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 129011.6562 - mse: 129011.6562 - mae: 239.7855 - val_loss: 117748.5781 - val_mse: 117748.5781 - val_mae: 229.1839\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 122563.7969 - mse: 122563.7969 - mae: 215.5639 - val_loss: 115602.9531 - val_mse: 115602.9531 - val_mae: 219.1074\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 242689.5938 - mse: 242689.5938 - mae: 287.0759 - val_loss: 233503.6094 - val_mse: 233503.6094 - val_mae: 300.7647\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 193722.0938 - mse: 193722.0938 - mae: 276.9282 - val_loss: 162131.1094 - val_mse: 162131.1094 - val_mae: 250.7866\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 191580.9375 - mse: 191580.9375 - mae: 267.0771 - val_loss: 183634.3125 - val_mse: 183634.3125 - val_mae: 234.9837\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 13s 87ms/step - loss: 184166.6562 - mse: 184166.6562 - mae: 243.0697 - val_loss: 205803.4688 - val_mse: 205803.4688 - val_mae: 259.8804\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 13s 89ms/step - loss: 156604.9844 - mse: 156604.9844 - mae: 222.0698 - val_loss: 222822.0000 - val_mse: 222822.0000 - val_mae: 242.5614\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 13s 89ms/step - loss: 253005.3594 - mse: 253005.3594 - mae: 280.8566 - val_loss: 302834.8125 - val_mse: 302834.8125 - val_mae: 310.7392\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 217183.2656 - mse: 217183.2656 - mae: 271.6412 - val_loss: 213389.8125 - val_mse: 213389.8125 - val_mae: 289.4922\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 13s 90ms/step - loss: 176960.0000 - mse: 176960.0000 - mae: 249.7120 - val_loss: 174874.9531 - val_mse: 174874.9531 - val_mae: 228.0533\n",
      "MAE LSTM fold n°5 = 1200.84\n",
      "🏋🏽‍♂️ improvement over baseline: -360.11 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°6 = 644.79\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 16s 91ms/step - loss: 52663924.0000 - mse: 52663924.0000 - mae: 6553.0361 - val_loss: 41355584.0000 - val_mse: 41355584.0000 - val_mae: 5525.3301\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 23091822.0000 - mse: 23091822.0000 - mae: 3726.9485 - val_loss: 13828682.0000 - val_mse: 13828682.0000 - val_mae: 2673.8435\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 10102846.0000 - mse: 10102846.0000 - mae: 2242.1497 - val_loss: 10866281.0000 - val_mse: 10866281.0000 - val_mae: 2378.5640\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 9700375.0000 - mse: 9700375.0000 - mae: 2232.2256 - val_loss: 10847182.0000 - val_mse: 10847182.0000 - val_mae: 2388.8713\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 9513385.0000 - mse: 9513385.0000 - mae: 2249.9343 - val_loss: 10832154.0000 - val_mse: 10832154.0000 - val_mae: 2402.7734\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 9659434.0000 - mse: 9659434.0000 - mae: 2244.8718 - val_loss: 10832634.0000 - val_mse: 10832634.0000 - val_mae: 2402.2175\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 9514142.0000 - mse: 9514142.0000 - mae: 2253.0144 - val_loss: 10831559.0000 - val_mse: 10831559.0000 - val_mae: 2403.6560\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 9513984.0000 - mse: 9513984.0000 - mae: 2252.9583 - val_loss: 10830966.0000 - val_mse: 10830966.0000 - val_mae: 2404.5308\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 9514222.0000 - mse: 9514222.0000 - mae: 2252.9666 - val_loss: 10830707.0000 - val_mse: 10830707.0000 - val_mae: 2404.9236\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 9514422.0000 - mse: 9514422.0000 - mae: 2252.9165 - val_loss: 10830527.0000 - val_mse: 10830527.0000 - val_mae: 2405.2056\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 9510413.0000 - mse: 9510413.0000 - mae: 2257.7634 - val_loss: 10829868.0000 - val_mse: 10829868.0000 - val_mae: 2406.2983\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 9514526.0000 - mse: 9514526.0000 - mae: 2252.8286 - val_loss: 10830341.0000 - val_mse: 10830341.0000 - val_mae: 2405.4902\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 9514801.0000 - mse: 9514801.0000 - mae: 2252.8064 - val_loss: 10830303.0000 - val_mse: 10830303.0000 - val_mae: 2405.5571\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 9514896.0000 - mse: 9514896.0000 - mae: 2252.7793 - val_loss: 10830285.0000 - val_mse: 10830285.0000 - val_mae: 2405.5813\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 9514980.0000 - mse: 9514980.0000 - mae: 2252.7573 - val_loss: 10830275.0000 - val_mse: 10830275.0000 - val_mae: 2405.5681\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 9514402.0000 - mse: 9514402.0000 - mae: 2250.6748 - val_loss: 10853346.0000 - val_mse: 10853346.0000 - val_mae: 2384.8701\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 6499003.5000 - mse: 6499003.5000 - mae: 1674.7649 - val_loss: 4592419.0000 - val_mse: 4592419.0000 - val_mae: 1291.9238\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 3571592.5000 - mse: 3571592.5000 - mae: 1186.1327 - val_loss: 4444945.0000 - val_mse: 4444945.0000 - val_mae: 1239.8815\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 3067097.2500 - mse: 3067097.2500 - mae: 1183.9236 - val_loss: 2519831.7500 - val_mse: 2519831.7500 - val_mae: 940.5535\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 16s 107ms/step - loss: 1479067.3750 - mse: 1479067.3750 - mae: 758.8183 - val_loss: 1378580.7500 - val_mse: 1378580.7500 - val_mae: 660.5902\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 799731.9375 - mse: 799731.9375 - mae: 545.3095 - val_loss: 1157416.2500 - val_mse: 1157416.2500 - val_mae: 761.9205\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 15s 104ms/step - loss: 539644.3750 - mse: 539644.3750 - mae: 454.0135 - val_loss: 547683.6250 - val_mse: 547683.6250 - val_mae: 409.7976\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 358564.6250 - mse: 358564.6250 - mae: 339.6885 - val_loss: 423447.4688 - val_mse: 423447.4688 - val_mae: 339.8559\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 322449.8750 - mse: 322449.8750 - mae: 312.7078 - val_loss: 393239.8438 - val_mse: 393239.8438 - val_mae: 322.6245\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 299200.2188 - mse: 299200.2188 - mae: 299.9754 - val_loss: 518227.5625 - val_mse: 518227.5625 - val_mae: 459.4505\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 302402.4688 - mse: 302402.4688 - mae: 313.6008 - val_loss: 362968.8438 - val_mse: 362968.8438 - val_mae: 315.0197\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 317393.1562 - mse: 317393.1562 - mae: 332.5337 - val_loss: 375702.1562 - val_mse: 375702.1562 - val_mae: 316.0855\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 267019.9688 - mse: 267019.9688 - mae: 301.7145 - val_loss: 291300.5312 - val_mse: 291300.5312 - val_mae: 323.0645\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 318993.0625 - mse: 318993.0625 - mae: 346.2443 - val_loss: 333204.0000 - val_mse: 333204.0000 - val_mae: 334.6837\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 14s 99ms/step - loss: 338702.9062 - mse: 338702.9062 - mae: 355.8397 - val_loss: 323691.6562 - val_mse: 323691.6562 - val_mae: 347.8761\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 251826.3906 - mse: 251826.3906 - mae: 305.3411 - val_loss: 287394.3125 - val_mse: 287394.3125 - val_mae: 316.8964\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 15s 102ms/step - loss: 226094.5312 - mse: 226094.5312 - mae: 287.3221 - val_loss: 225879.2969 - val_mse: 225879.2969 - val_mae: 283.3399\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 203532.0781 - mse: 203532.0781 - mae: 280.0137 - val_loss: 215859.9062 - val_mse: 215859.9062 - val_mae: 269.4015\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 191786.8438 - mse: 191786.8438 - mae: 272.7232 - val_loss: 273025.8750 - val_mse: 273025.8750 - val_mae: 328.1444\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 263362.5312 - mse: 263362.5312 - mae: 330.6786 - val_loss: 272267.2188 - val_mse: 272267.2188 - val_mae: 328.3728\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 214640.1250 - mse: 214640.1250 - mae: 287.5728 - val_loss: 266059.9062 - val_mse: 266059.9062 - val_mae: 299.4660\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 239489.0625 - mse: 239489.0625 - mae: 296.7017 - val_loss: 251029.7500 - val_mse: 251029.7500 - val_mae: 288.4597\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 15s 100ms/step - loss: 164427.5156 - mse: 164427.5156 - mae: 244.3161 - val_loss: 198415.5938 - val_mse: 198415.5938 - val_mae: 262.4731\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 146811.1094 - mse: 146811.1094 - mae: 242.2878 - val_loss: 154408.0000 - val_mse: 154408.0000 - val_mae: 237.7515\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 148002.6406 - mse: 148002.6406 - mae: 237.8538 - val_loss: 220223.5625 - val_mse: 220223.5625 - val_mae: 302.7751\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 14s 98ms/step - loss: 134382.4219 - mse: 134382.4219 - mae: 240.7241 - val_loss: 165683.7812 - val_mse: 165683.7812 - val_mae: 259.9398\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 207473.8125 - mse: 207473.8125 - mae: 286.2926 - val_loss: 150894.8906 - val_mse: 150894.8906 - val_mae: 235.9182\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 16s 109ms/step - loss: 141564.7969 - mse: 141564.7969 - mae: 245.9682 - val_loss: 119627.2500 - val_mse: 119627.2500 - val_mae: 225.4953\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 139933.2188 - mse: 139933.2188 - mae: 247.9606 - val_loss: 108918.2344 - val_mse: 108918.2344 - val_mae: 224.0634\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 135743.0781 - mse: 135743.0781 - mae: 245.5944 - val_loss: 152077.3750 - val_mse: 152077.3750 - val_mae: 273.3177\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 123745.1953 - mse: 123745.1953 - mae: 240.1046 - val_loss: 108326.2188 - val_mse: 108326.2188 - val_mae: 216.3896\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 116531.0391 - mse: 116531.0391 - mae: 219.3980 - val_loss: 127058.3047 - val_mse: 127058.3047 - val_mae: 226.0214\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 159261.0469 - mse: 159261.0469 - mae: 250.0297 - val_loss: 195338.3438 - val_mse: 195338.3438 - val_mae: 277.4497\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 163434.7031 - mse: 163434.7031 - mae: 271.1771 - val_loss: 163236.9844 - val_mse: 163236.9844 - val_mae: 276.1338\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 199160.0625 - mse: 199160.0625 - mae: 293.4347 - val_loss: 211941.9844 - val_mse: 211941.9844 - val_mae: 309.9263\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 14s 97ms/step - loss: 177725.1094 - mse: 177725.1094 - mae: 273.6488 - val_loss: 160029.8906 - val_mse: 160029.8906 - val_mae: 269.3698\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 181560.7812 - mse: 181560.7812 - mae: 274.4332 - val_loss: 157695.0469 - val_mse: 157695.0469 - val_mae: 239.4028\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 199478.1562 - mse: 199478.1562 - mae: 282.0925 - val_loss: 160433.8594 - val_mse: 160433.8594 - val_mae: 261.4018\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 156992.1875 - mse: 156992.1875 - mae: 254.1995 - val_loss: 242945.2031 - val_mse: 242945.2031 - val_mae: 266.2926\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 164307.0000 - mse: 164307.0000 - mae: 265.2255 - val_loss: 145664.7500 - val_mse: 145664.7500 - val_mae: 252.8251\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 121804.6875 - mse: 121804.6875 - mae: 226.4641 - val_loss: 95903.8984 - val_mse: 95903.8984 - val_mae: 202.3341\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 124939.1484 - mse: 124939.1484 - mae: 233.4060 - val_loss: 133168.0781 - val_mse: 133168.0781 - val_mae: 243.6224\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 138333.1406 - mse: 138333.1406 - mae: 250.2890 - val_loss: 135653.5469 - val_mse: 135653.5469 - val_mae: 233.9719\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 118747.0234 - mse: 118747.0234 - mae: 213.3192 - val_loss: 114084.1406 - val_mse: 114084.1406 - val_mae: 197.4846\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 148750.0938 - mse: 148750.0938 - mae: 235.2352 - val_loss: 128189.0547 - val_mse: 128189.0547 - val_mae: 246.3642\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 142733.5156 - mse: 142733.5156 - mae: 234.9523 - val_loss: 146140.1875 - val_mse: 146140.1875 - val_mae: 230.7997\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 192037.1562 - mse: 192037.1562 - mae: 271.1650 - val_loss: 145046.9375 - val_mse: 145046.9375 - val_mae: 248.7867\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 154172.3750 - mse: 154172.3750 - mae: 251.5743 - val_loss: 163777.2656 - val_mse: 163777.2656 - val_mae: 227.6416\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 14s 93ms/step - loss: 184116.6719 - mse: 184116.6719 - mae: 271.3324 - val_loss: 153187.4062 - val_mse: 153187.4062 - val_mae: 263.8597\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 14s 95ms/step - loss: 127606.8516 - mse: 127606.8516 - mae: 225.9315 - val_loss: 105232.1328 - val_mse: 105232.1328 - val_mae: 198.6979\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 16s 112ms/step - loss: 123980.2656 - mse: 123980.2656 - mae: 226.0464 - val_loss: 112463.2812 - val_mse: 112463.2812 - val_mae: 208.6801\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 13s 92ms/step - loss: 114177.1172 - mse: 114177.1172 - mae: 218.2530 - val_loss: 122576.9297 - val_mse: 122576.9297 - val_mae: 219.2952\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 163710.8750 - mse: 163710.8750 - mae: 248.5947 - val_loss: 158051.8281 - val_mse: 158051.8281 - val_mae: 264.2570\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 14s 96ms/step - loss: 149095.2344 - mse: 149095.2344 - mae: 247.7261 - val_loss: 106266.5469 - val_mse: 106266.5469 - val_mae: 210.6150\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 13s 91ms/step - loss: 140557.0469 - mse: 140557.0469 - mae: 241.8450 - val_loss: 154976.0000 - val_mse: 154976.0000 - val_mae: 255.2366\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 14s 94ms/step - loss: 169998.2969 - mse: 169998.2969 - mae: 274.9361 - val_loss: 138612.4219 - val_mse: 138612.4219 - val_mae: 244.8032\n",
      "MAE LSTM fold n°6 = 4621.95\n",
      "🏋🏽‍♂️ improvement over baseline: -616.81 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°7 = 1475.07\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 16s 86ms/step - loss: 56862468.0000 - mse: 56862468.0000 - mae: 6879.4932 - val_loss: 53287316.0000 - val_mse: 53287316.0000 - val_mae: 6516.2095\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 39581532.0000 - mse: 39581532.0000 - mae: 5453.3330 - val_loss: 30416490.0000 - val_mse: 30416490.0000 - val_mae: 4426.0781\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 19464904.0000 - mse: 19464904.0000 - mae: 3310.8745 - val_loss: 14872492.0000 - val_mse: 14872492.0000 - val_mae: 2794.7622\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 12s 86ms/step - loss: 10856620.0000 - mse: 10856620.0000 - mae: 2325.6816 - val_loss: 11174054.0000 - val_mse: 11174054.0000 - val_mae: 2334.4666\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9572759.0000 - mse: 9572759.0000 - mae: 2207.8655 - val_loss: 10857013.0000 - val_mse: 10857013.0000 - val_mae: 2383.0530\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9511336.0000 - mse: 9511336.0000 - mae: 2245.7698 - val_loss: 10837135.0000 - val_mse: 10837135.0000 - val_mae: 2397.0073\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 12323324.0000 - mse: 12323324.0000 - mae: 2539.5173 - val_loss: 12231391.0000 - val_mse: 12231391.0000 - val_mae: 2458.9226\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9803813.0000 - mse: 9803813.0000 - mae: 2202.4043 - val_loss: 10879538.0000 - val_mse: 10879538.0000 - val_mae: 2373.5098\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513767.0000 - mse: 9513767.0000 - mae: 2241.4531 - val_loss: 10837243.0000 - val_mse: 10837243.0000 - val_mae: 2396.9001\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9511287.0000 - mse: 9511287.0000 - mae: 2252.3359 - val_loss: 10834726.0000 - val_mse: 10834726.0000 - val_mae: 2399.5793\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9511634.0000 - mse: 9511634.0000 - mae: 2253.2058 - val_loss: 10834209.0000 - val_mse: 10834209.0000 - val_mae: 2400.1804\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9511908.0000 - mse: 9511908.0000 - mae: 2253.2395 - val_loss: 10833835.0000 - val_mse: 10833835.0000 - val_mae: 2400.6243\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9512164.0000 - mse: 9512164.0000 - mae: 2253.2212 - val_loss: 10833492.0000 - val_mse: 10833492.0000 - val_mae: 2401.0410\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9512404.0000 - mse: 9512404.0000 - mae: 2253.2014 - val_loss: 10833167.0000 - val_mse: 10833167.0000 - val_mae: 2401.4341\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9511932.0000 - mse: 9511932.0000 - mae: 2253.0203 - val_loss: 10847759.0000 - val_mse: 10847759.0000 - val_mae: 2374.8223\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9520734.0000 - mse: 9520734.0000 - mae: 2254.3049 - val_loss: 10832521.0000 - val_mse: 10832521.0000 - val_mae: 2402.2842\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9512948.0000 - mse: 9512948.0000 - mae: 2253.1643 - val_loss: 10832411.0000 - val_mse: 10832411.0000 - val_mae: 2402.4326\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513100.0000 - mse: 9513100.0000 - mae: 2253.1018 - val_loss: 10832207.0000 - val_mse: 10832207.0000 - val_mae: 2402.7075\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513232.0000 - mse: 9513232.0000 - mae: 2253.0801 - val_loss: 10832024.0000 - val_mse: 10832024.0000 - val_mae: 2402.9563\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9513350.0000 - mse: 9513350.0000 - mae: 2253.0569 - val_loss: 10831864.0000 - val_mse: 10831864.0000 - val_mae: 2403.1792\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513453.0000 - mse: 9513453.0000 - mae: 2253.0369 - val_loss: 10831728.0000 - val_mse: 10831728.0000 - val_mae: 2403.3770\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513552.0000 - mse: 9513552.0000 - mae: 2253.0195 - val_loss: 10831602.0000 - val_mse: 10831602.0000 - val_mae: 2403.5510\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513631.0000 - mse: 9513631.0000 - mae: 2253.0027 - val_loss: 10831497.0000 - val_mse: 10831497.0000 - val_mae: 2403.7043\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 9513697.0000 - mse: 9513697.0000 - mae: 2252.9863 - val_loss: 10831405.0000 - val_mse: 10831405.0000 - val_mae: 2403.8381\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 13s 88ms/step - loss: 9513762.0000 - mse: 9513762.0000 - mae: 2252.9719 - val_loss: 10831317.0000 - val_mse: 10831317.0000 - val_mae: 2403.9534\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9516230.0000 - mse: 9516230.0000 - mae: 2249.4260 - val_loss: 10847529.0000 - val_mse: 10847529.0000 - val_mae: 2388.6021\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9521914.0000 - mse: 9521914.0000 - mae: 2226.3030 - val_loss: 10839456.0000 - val_mse: 10839456.0000 - val_mae: 2394.8345\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9520610.0000 - mse: 9520610.0000 - mae: 2242.9719 - val_loss: 10831901.0000 - val_mse: 10831901.0000 - val_mae: 2403.1272\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513883.0000 - mse: 9513883.0000 - mae: 2252.8250 - val_loss: 10831287.0000 - val_mse: 10831287.0000 - val_mae: 2404.0125\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513824.0000 - mse: 9513824.0000 - mae: 2252.9519 - val_loss: 10831260.0000 - val_mse: 10831260.0000 - val_mae: 2404.0500\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9513920.0000 - mse: 9513920.0000 - mae: 2252.9526 - val_loss: 10831219.0000 - val_mse: 10831219.0000 - val_mae: 2404.1113\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513875.0000 - mse: 9513875.0000 - mae: 2252.9377 - val_loss: 10831200.0000 - val_mse: 10831200.0000 - val_mae: 2404.1421\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513890.0000 - mse: 9513890.0000 - mae: 2252.9304 - val_loss: 10831175.0000 - val_mse: 10831175.0000 - val_mae: 2404.1794\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513910.0000 - mse: 9513910.0000 - mae: 2252.9246 - val_loss: 10831152.0000 - val_mse: 10831152.0000 - val_mae: 2404.2124\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513921.0000 - mse: 9513921.0000 - mae: 2252.9204 - val_loss: 10831135.0000 - val_mse: 10831135.0000 - val_mae: 2404.2410\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513941.0000 - mse: 9513941.0000 - mae: 2252.9165 - val_loss: 10831118.0000 - val_mse: 10831118.0000 - val_mae: 2404.2661\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 13s 86ms/step - loss: 9513952.0000 - mse: 9513952.0000 - mae: 2252.9133 - val_loss: 10831103.0000 - val_mse: 10831103.0000 - val_mae: 2404.2881\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513961.0000 - mse: 9513961.0000 - mae: 2252.9099 - val_loss: 10831092.0000 - val_mse: 10831092.0000 - val_mae: 2404.3057\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513973.0000 - mse: 9513973.0000 - mae: 2252.9070 - val_loss: 10831080.0000 - val_mse: 10831080.0000 - val_mae: 2404.3230\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513976.0000 - mse: 9513976.0000 - mae: 2252.9045 - val_loss: 10831071.0000 - val_mse: 10831071.0000 - val_mae: 2404.3362\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9513983.0000 - mse: 9513983.0000 - mae: 2252.9026 - val_loss: 10831062.0000 - val_mse: 10831062.0000 - val_mae: 2404.3496\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513989.0000 - mse: 9513989.0000 - mae: 2252.9016 - val_loss: 10831056.0000 - val_mse: 10831056.0000 - val_mae: 2404.3596\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9513994.0000 - mse: 9513994.0000 - mae: 2252.8999 - val_loss: 10831052.0000 - val_mse: 10831052.0000 - val_mae: 2404.3682\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9513998.0000 - mse: 9513998.0000 - mae: 2252.8979 - val_loss: 10831041.0000 - val_mse: 10831041.0000 - val_mae: 2404.3774\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9514003.0000 - mse: 9514003.0000 - mae: 2252.8975 - val_loss: 10831039.0000 - val_mse: 10831039.0000 - val_mae: 2404.3835\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9514003.0000 - mse: 9514003.0000 - mae: 2252.8958 - val_loss: 10831036.0000 - val_mse: 10831036.0000 - val_mae: 2404.3892\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 9514025.0000 - mse: 9514025.0000 - mae: 2252.9001 - val_loss: 10831031.0000 - val_mse: 10831031.0000 - val_mae: 2404.3958\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 12s 85ms/step - loss: 9514011.0000 - mse: 9514011.0000 - mae: 2252.8940 - val_loss: 10831029.0000 - val_mse: 10831029.0000 - val_mae: 2404.3989\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 9514011.0000 - mse: 9514011.0000 - mae: 2252.8928 - val_loss: 10831026.0000 - val_mse: 10831026.0000 - val_mae: 2404.4021\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 9514014.0000 - mse: 9514014.0000 - mae: 2252.8928 - val_loss: 10831024.0000 - val_mse: 10831024.0000 - val_mae: 2404.4065\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 8739083.0000 - mse: 8739083.0000 - mae: 2087.8098 - val_loss: 7949629.0000 - val_mse: 7949629.0000 - val_mae: 1790.3694\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 3840372.7500 - mse: 3840372.7500 - mae: 1169.1754 - val_loss: 2763494.0000 - val_mse: 2763494.0000 - val_mae: 880.3145\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 1619158.7500 - mse: 1619158.7500 - mae: 683.9863 - val_loss: 1593902.2500 - val_mse: 1593902.2500 - val_mae: 684.7689\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 990469.6250 - mse: 990469.6250 - mae: 569.1890 - val_loss: 909280.2500 - val_mse: 909280.2500 - val_mae: 470.4623\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 598162.9375 - mse: 598162.9375 - mae: 431.3849 - val_loss: 670443.7500 - val_mse: 670443.7500 - val_mae: 439.0871\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 520670.6562 - mse: 520670.6562 - mae: 412.2609 - val_loss: 595145.2500 - val_mse: 595145.2500 - val_mae: 424.7869\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 431837.5312 - mse: 431837.5312 - mae: 387.8960 - val_loss: 458682.2188 - val_mse: 458682.2188 - val_mae: 370.1840\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 366543.5312 - mse: 366543.5312 - mae: 343.0330 - val_loss: 461297.5625 - val_mse: 461297.5625 - val_mae: 359.6333\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 451526.5312 - mse: 451526.5312 - mae: 419.0129 - val_loss: 412049.4688 - val_mse: 412049.4688 - val_mae: 368.1089\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 397119.4375 - mse: 397119.4375 - mae: 355.8295 - val_loss: 574992.0000 - val_mse: 574992.0000 - val_mae: 390.5310\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 314599.7188 - mse: 314599.7188 - mae: 328.2026 - val_loss: 309756.0312 - val_mse: 309756.0312 - val_mae: 306.7480\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 303680.0000 - mse: 303680.0000 - mae: 344.5558 - val_loss: 308794.0938 - val_mse: 308794.0938 - val_mae: 344.7455\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 263386.7188 - mse: 263386.7188 - mae: 323.8457 - val_loss: 294478.8125 - val_mse: 294478.8125 - val_mae: 335.3592\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 379837.5000 - mse: 379837.5000 - mae: 392.1535 - val_loss: 299040.8750 - val_mse: 299040.8750 - val_mae: 352.2160\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 320860.2188 - mse: 320860.2188 - mae: 364.0495 - val_loss: 281227.7812 - val_mse: 281227.7812 - val_mae: 310.8090\n",
      "Epoch 66/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 249273.9688 - mse: 249273.9688 - mae: 307.8151 - val_loss: 354763.7188 - val_mse: 354763.7188 - val_mae: 325.5939\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 339196.2812 - mse: 339196.2812 - mae: 368.6966 - val_loss: 320374.4062 - val_mse: 320374.4062 - val_mae: 366.3958\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 275523.2812 - mse: 275523.2812 - mae: 338.3173 - val_loss: 367645.4688 - val_mse: 367645.4688 - val_mae: 362.4157\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 242862.1094 - mse: 242862.1094 - mae: 316.7688 - val_loss: 271742.4688 - val_mse: 271742.4688 - val_mae: 361.1217\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 259087.9688 - mse: 259087.9688 - mae: 349.0750 - val_loss: 234348.9219 - val_mse: 234348.9219 - val_mae: 318.6088\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 222012.8906 - mse: 222012.8906 - mae: 299.2234 - val_loss: 244284.0469 - val_mse: 244284.0469 - val_mae: 318.2306\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 251748.6719 - mse: 251748.6719 - mae: 328.3315 - val_loss: 281186.1250 - val_mse: 281186.1250 - val_mae: 364.9391\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 221683.8125 - mse: 221683.8125 - mae: 304.3380 - val_loss: 182539.5312 - val_mse: 182539.5312 - val_mae: 265.2672\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 218476.3594 - mse: 218476.3594 - mae: 309.6702 - val_loss: 208428.0938 - val_mse: 208428.0938 - val_mae: 286.4709\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 221241.5938 - mse: 221241.5938 - mae: 290.0147 - val_loss: 219151.8594 - val_mse: 219151.8594 - val_mae: 271.2107\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 259179.6719 - mse: 259179.6719 - mae: 318.2282 - val_loss: 221085.2812 - val_mse: 221085.2812 - val_mae: 292.0258\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 242172.5469 - mse: 242172.5469 - mae: 300.6615 - val_loss: 266257.3750 - val_mse: 266257.3750 - val_mae: 299.7764\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 217393.8281 - mse: 217393.8281 - mae: 280.6580 - val_loss: 219175.2188 - val_mse: 219175.2188 - val_mae: 262.5645\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 191347.1094 - mse: 191347.1094 - mae: 257.9216 - val_loss: 159956.5156 - val_mse: 159956.5156 - val_mae: 233.7191\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 171744.0000 - mse: 171744.0000 - mae: 256.0955 - val_loss: 192474.4688 - val_mse: 192474.4688 - val_mae: 289.3764\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 216251.4844 - mse: 216251.4844 - mae: 295.6978 - val_loss: 196127.3438 - val_mse: 196127.3438 - val_mae: 284.5172\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 152406.1562 - mse: 152406.1562 - mae: 254.7628 - val_loss: 113763.3516 - val_mse: 113763.3516 - val_mae: 212.0739\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 120415.5078 - mse: 120415.5078 - mae: 226.0025 - val_loss: 154746.6875 - val_mse: 154746.6875 - val_mae: 247.3705\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 109694.8125 - mse: 109694.8125 - mae: 219.4153 - val_loss: 114702.9297 - val_mse: 114702.9297 - val_mae: 235.3036\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 118280.0625 - mse: 118280.0625 - mae: 232.6635 - val_loss: 120664.2891 - val_mse: 120664.2891 - val_mae: 209.9742\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 92772.2109 - mse: 92772.2109 - mae: 197.1072 - val_loss: 95266.8281 - val_mse: 95266.8281 - val_mae: 205.5423\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 87857.6016 - mse: 87857.6016 - mae: 198.2471 - val_loss: 91729.7422 - val_mse: 91729.7422 - val_mae: 199.2993\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 93518.5625 - mse: 93518.5625 - mae: 200.3955 - val_loss: 129528.6328 - val_mse: 129528.6328 - val_mae: 240.5765\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 130295.9375 - mse: 130295.9375 - mae: 229.8995 - val_loss: 118663.3281 - val_mse: 118663.3281 - val_mae: 210.7939\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 163391.0781 - mse: 163391.0781 - mae: 250.3702 - val_loss: 246302.3750 - val_mse: 246302.3750 - val_mae: 305.6200\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 175406.9062 - mse: 175406.9062 - mae: 274.8265 - val_loss: 200170.1406 - val_mse: 200170.1406 - val_mae: 273.7439\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 176421.0000 - mse: 176421.0000 - mae: 259.3140 - val_loss: 142268.6094 - val_mse: 142268.6094 - val_mae: 241.4432\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 119958.9062 - mse: 119958.9062 - mae: 223.4195 - val_loss: 133569.3438 - val_mse: 133569.3438 - val_mae: 217.1757\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 125711.7891 - mse: 125711.7891 - mae: 219.4776 - val_loss: 117700.9297 - val_mse: 117700.9297 - val_mae: 220.4257\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 114818.3594 - mse: 114818.3594 - mae: 219.1561 - val_loss: 281235.8125 - val_mse: 281235.8125 - val_mae: 268.9156\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 183108.0625 - mse: 183108.0625 - mae: 273.1306 - val_loss: 172955.3906 - val_mse: 172955.3906 - val_mae: 291.7152\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 259055.9219 - mse: 259055.9219 - mae: 341.8332 - val_loss: 329437.5938 - val_mse: 329437.5938 - val_mae: 381.2685\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 188598.9844 - mse: 188598.9844 - mae: 296.9327 - val_loss: 125226.7734 - val_mse: 125226.7734 - val_mae: 241.1106\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 12s 84ms/step - loss: 131138.8594 - mse: 131138.8594 - mae: 245.6158 - val_loss: 112617.9297 - val_mse: 112617.9297 - val_mae: 226.8784\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 12s 83ms/step - loss: 121205.1406 - mse: 121205.1406 - mae: 236.4719 - val_loss: 115551.3359 - val_mse: 115551.3359 - val_mae: 235.9274\n",
      "MAE LSTM fold n°7 = 22071.13\n",
      "🏋🏽‍♂️ improvement over baseline: -1396.28 % \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MAE baseline fold n°8 = 1981.5\n",
      "Epoch 1/100\n",
      "146/146 [==============================] - 14s 79ms/step - loss: 55552916.0000 - mse: 55552916.0000 - mae: 6781.1128 - val_loss: 49726968.0000 - val_mse: 49726968.0000 - val_mae: 6237.0381\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 33887012.0000 - mse: 33887012.0000 - mae: 4880.4189 - val_loss: 23451618.0000 - val_mse: 23451618.0000 - val_mae: 3585.1250\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 14446675.0000 - mse: 14446675.0000 - mae: 2752.6626 - val_loss: 12020983.0000 - val_mse: 12020983.0000 - val_mae: 2426.4363\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 9768289.0000 - mse: 9768289.0000 - mae: 2198.6221 - val_loss: 10878747.0000 - val_mse: 10878747.0000 - val_mae: 2373.7888\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 9513682.0000 - mse: 9513682.0000 - mae: 2241.7368 - val_loss: 10836697.0000 - val_mse: 10836697.0000 - val_mae: 2397.4524\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 9511550.0000 - mse: 9511550.0000 - mae: 2252.4568 - val_loss: 10834161.0000 - val_mse: 10834161.0000 - val_mae: 2400.2385\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 9902032.0000 - mse: 9902032.0000 - mae: 2269.3625 - val_loss: 10983277.0000 - val_mse: 10983277.0000 - val_mae: 2347.0288\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 8284106.5000 - mse: 8284106.5000 - mae: 2008.8959 - val_loss: 7002622.0000 - val_mse: 7002622.0000 - val_mae: 1829.7739\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 3943300.7500 - mse: 3943300.7500 - mae: 1185.1904 - val_loss: 3414085.5000 - val_mse: 3414085.5000 - val_mae: 971.9019\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 2010782.5000 - mse: 2010782.5000 - mae: 735.8638 - val_loss: 1931333.5000 - val_mse: 1931333.5000 - val_mae: 711.6251\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 1238866.2500 - mse: 1238866.2500 - mae: 616.7223 - val_loss: 1283092.7500 - val_mse: 1283092.7500 - val_mae: 567.1127\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 11s 76ms/step - loss: 838833.6875 - mse: 838833.6875 - mae: 516.3378 - val_loss: 963538.5625 - val_mse: 963538.5625 - val_mae: 522.2197\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 720013.1875 - mse: 720013.1875 - mae: 514.0248 - val_loss: 861921.6875 - val_mse: 861921.6875 - val_mae: 556.4985\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 675365.6875 - mse: 675365.6875 - mae: 509.5401 - val_loss: 804863.2500 - val_mse: 804863.2500 - val_mae: 527.8118\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 527477.8750 - mse: 527477.8750 - mae: 425.2773 - val_loss: 550625.6250 - val_mse: 550625.6250 - val_mae: 412.1889\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 453279.7500 - mse: 453279.7500 - mae: 412.1102 - val_loss: 472689.7188 - val_mse: 472689.7188 - val_mae: 384.4265\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 424333.6250 - mse: 424333.6250 - mae: 410.7953 - val_loss: 386185.8438 - val_mse: 386185.8438 - val_mae: 350.8458\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 340160.2500 - mse: 340160.2500 - mae: 343.2330 - val_loss: 372916.4062 - val_mse: 372916.4062 - val_mae: 340.8268\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 295126.7188 - mse: 295126.7188 - mae: 338.9796 - val_loss: 319232.6250 - val_mse: 319232.6250 - val_mae: 338.6338\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 270882.9375 - mse: 270882.9375 - mae: 313.4259 - val_loss: 313330.6250 - val_mse: 313330.6250 - val_mae: 333.0587\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 12s 82ms/step - loss: 234607.4375 - mse: 234607.4375 - mae: 299.1923 - val_loss: 295332.7812 - val_mse: 295332.7812 - val_mae: 341.9456\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 233147.9062 - mse: 233147.9062 - mae: 308.9200 - val_loss: 221391.8750 - val_mse: 221391.8750 - val_mae: 288.7224\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 11s 76ms/step - loss: 209777.1406 - mse: 209777.1406 - mae: 295.3226 - val_loss: 231035.3438 - val_mse: 231035.3438 - val_mae: 322.0372\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 190266.1875 - mse: 190266.1875 - mae: 289.7674 - val_loss: 173533.2969 - val_mse: 173533.2969 - val_mae: 261.7475\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 196268.1875 - mse: 196268.1875 - mae: 300.7560 - val_loss: 211459.0156 - val_mse: 211459.0156 - val_mae: 302.6606\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 228372.1719 - mse: 228372.1719 - mae: 304.6674 - val_loss: 336469.3125 - val_mse: 336469.3125 - val_mae: 345.5334\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 220177.9062 - mse: 220177.9062 - mae: 296.3105 - val_loss: 202644.3594 - val_mse: 202644.3594 - val_mae: 272.5801\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 185600.1250 - mse: 185600.1250 - mae: 279.9957 - val_loss: 212584.7500 - val_mse: 212584.7500 - val_mae: 320.7603\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 168234.5938 - mse: 168234.5938 - mae: 257.5250 - val_loss: 207142.3438 - val_mse: 207142.3438 - val_mae: 287.1742\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 200561.8125 - mse: 200561.8125 - mae: 294.9823 - val_loss: 173651.3438 - val_mse: 173651.3438 - val_mae: 261.8826\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 182939.7969 - mse: 182939.7969 - mae: 280.9667 - val_loss: 147523.0156 - val_mse: 147523.0156 - val_mae: 243.4142\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 168123.6719 - mse: 168123.6719 - mae: 262.8922 - val_loss: 172380.0469 - val_mse: 172380.0469 - val_mae: 264.1801\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 148363.3281 - mse: 148363.3281 - mae: 247.1136 - val_loss: 151506.3906 - val_mse: 151506.3906 - val_mae: 257.4526\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 134181.0938 - mse: 134181.0938 - mae: 230.5696 - val_loss: 143084.6406 - val_mse: 143084.6406 - val_mae: 232.3160\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 11s 77ms/step - loss: 126305.7578 - mse: 126305.7578 - mae: 234.9398 - val_loss: 122135.9688 - val_mse: 122135.9688 - val_mae: 224.6464\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 194363.7500 - mse: 194363.7500 - mae: 284.4615 - val_loss: 254754.0312 - val_mse: 254754.0312 - val_mae: 330.2968\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 369775.3125 - mse: 369775.3125 - mae: 398.8310 - val_loss: 342525.0000 - val_mse: 342525.0000 - val_mae: 379.5768\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 342592.0000 - mse: 342592.0000 - mae: 407.2012 - val_loss: 414545.7500 - val_mse: 414545.7500 - val_mae: 478.6926\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 234682.1250 - mse: 234682.1250 - mae: 324.5326 - val_loss: 246115.8281 - val_mse: 246115.8281 - val_mae: 349.7768\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 234653.4844 - mse: 234653.4844 - mae: 317.4711 - val_loss: 281162.1250 - val_mse: 281162.1250 - val_mae: 309.4728\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 193915.5625 - mse: 193915.5625 - mae: 298.2214 - val_loss: 189936.9688 - val_mse: 189936.9688 - val_mae: 276.2498\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 12s 80ms/step - loss: 165099.1719 - mse: 165099.1719 - mae: 265.6664 - val_loss: 179307.2500 - val_mse: 179307.2500 - val_mae: 286.9081\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 188038.9688 - mse: 188038.9688 - mae: 282.3654 - val_loss: 210961.0781 - val_mse: 210961.0781 - val_mae: 318.3212\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 226777.0469 - mse: 226777.0469 - mae: 320.8514 - val_loss: 285826.9375 - val_mse: 285826.9375 - val_mae: 378.4079\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 11s 79ms/step - loss: 153651.9688 - mse: 153651.9688 - mae: 256.7301 - val_loss: 149707.2031 - val_mse: 149707.2031 - val_mae: 287.9173\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 154512.4219 - mse: 154512.4219 - mae: 263.8028 - val_loss: 182407.9062 - val_mse: 182407.9062 - val_mae: 286.9407\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 12s 81ms/step - loss: 210571.7031 - mse: 210571.7031 - mae: 285.0255 - val_loss: 234917.4844 - val_mse: 234917.4844 - val_mae: 301.4706\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 207166.4844 - mse: 207166.4844 - mae: 289.7847 - val_loss: 221747.1562 - val_mse: 221747.1562 - val_mae: 287.5783\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 11s 78ms/step - loss: 228656.4375 - mse: 228656.4375 - mae: 299.8566 - val_loss: 157991.3438 - val_mse: 157991.3438 - val_mae: 258.0672\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 12s 79ms/step - loss: 275899.1562 - mse: 275899.1562 - mae: 356.1306 - val_loss: 273325.1875 - val_mse: 273325.1875 - val_mae: 369.2030\n",
      "MAE LSTM fold n°8 = 25626.35\n",
      "🏋🏽‍♂️ improvement over baseline: -1193.28 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mae_baselines, mae_lstms = cross_validate_baseline_and_lstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average percentage improvement over baseline = -494.00000000000006%\n"
     ]
    }
   ],
   "source": [
    "print(f\"average percentage improvement over baseline = {round(np.mean(1 - (np.array(mae_lstms)/np.array(mae_baselines))),2)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_market_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
